{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 使用one-hot 初始编码进行训练和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decode_review = [reverse_word_index.get(i-3, '?') for i in train_data[0]]\n",
    "decode_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "15000/15000 [==============================] - 1s 84us/step - loss: 0.5326 - acc: 0.7917 - val_loss: 0.4064 - val_acc: 0.8700\n",
      "Epoch 2/20\n",
      "15000/15000 [==============================] - 1s 71us/step - loss: 0.3258 - acc: 0.8987 - val_loss: 0.3154 - val_acc: 0.8851\n",
      "Epoch 3/20\n",
      "15000/15000 [==============================] - 1s 72us/step - loss: 0.2357 - acc: 0.9245 - val_loss: 0.2827 - val_acc: 0.8899\n",
      "Epoch 4/20\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.1866 - acc: 0.9397 - val_loss: 0.2862 - val_acc: 0.8838\n",
      "Epoch 5/20\n",
      "15000/15000 [==============================] - 1s 79us/step - loss: 0.1502 - acc: 0.9521 - val_loss: 0.2768 - val_acc: 0.8887\n",
      "Epoch 6/20\n",
      "15000/15000 [==============================] - 1s 75us/step - loss: 0.1256 - acc: 0.9615 - val_loss: 0.3117 - val_acc: 0.8800\n",
      "Epoch 7/20\n",
      "15000/15000 [==============================] - 1s 76us/step - loss: 0.1045 - acc: 0.9687 - val_loss: 0.3120 - val_acc: 0.8837\n",
      "Epoch 8/20\n",
      "15000/15000 [==============================] - 1s 72us/step - loss: 0.0882 - acc: 0.9730 - val_loss: 0.3210 - val_acc: 0.8817\n",
      "Epoch 9/20\n",
      "15000/15000 [==============================] - 1s 75us/step - loss: 0.0736 - acc: 0.9793 - val_loss: 0.3549 - val_acc: 0.8819\n",
      "Epoch 10/20\n",
      "15000/15000 [==============================] - 1s 78us/step - loss: 0.0595 - acc: 0.9851 - val_loss: 0.3970 - val_acc: 0.8743\n",
      "Epoch 11/20\n",
      "15000/15000 [==============================] - 1s 76us/step - loss: 0.0508 - acc: 0.9865 - val_loss: 0.3904 - val_acc: 0.8783\n",
      "Epoch 12/20\n",
      "15000/15000 [==============================] - 1s 81us/step - loss: 0.0396 - acc: 0.9914 - val_loss: 0.4198 - val_acc: 0.8742\n",
      "Epoch 13/20\n",
      "15000/15000 [==============================] - 1s 76us/step - loss: 0.0336 - acc: 0.9933 - val_loss: 0.4488 - val_acc: 0.8726\n",
      "Epoch 14/20\n",
      "15000/15000 [==============================] - 1s 78us/step - loss: 0.0278 - acc: 0.9943 - val_loss: 0.4748 - val_acc: 0.8736\n",
      "Epoch 15/20\n",
      "15000/15000 [==============================] - 1s 78us/step - loss: 0.0239 - acc: 0.9947 - val_loss: 0.5064 - val_acc: 0.8706\n",
      "Epoch 16/20\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.0165 - acc: 0.9979 - val_loss: 0.5909 - val_acc: 0.8561\n",
      "Epoch 17/20\n",
      "15000/15000 [==============================] - 1s 78us/step - loss: 0.0125 - acc: 0.9988 - val_loss: 0.5645 - val_acc: 0.8675\n",
      "Epoch 18/20\n",
      "15000/15000 [==============================] - 1s 75us/step - loss: 0.0113 - acc: 0.9987 - val_loss: 0.6023 - val_acc: 0.8657\n",
      "Epoch 19/20\n",
      "15000/15000 [==============================] - 1s 75us/step - loss: 0.0086 - acc: 0.9992 - val_loss: 0.6370 - val_acc: 0.8645\n",
      "Epoch 20/20\n",
      "15000/15000 [==============================] - 1s 67us/step - loss: 0.0100 - acc: 0.9979 - val_loss: 0.6724 - val_acc: 0.8658\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(10000, )))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]\n",
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=20,\n",
    "                   batch_size=512,\n",
    "                   validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 找到最佳情况为第五个epoch并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "15000/15000 [==============================] - 1s 77us/step - loss: 0.0027 - acc: 0.9993 - val_loss: 0.9689 - val_acc: 0.8603\n",
      "Epoch 2/5\n",
      "15000/15000 [==============================] - 1s 69us/step - loss: 2.9645e-04 - acc: 1.0000 - val_loss: 0.9783 - val_acc: 0.8590\n",
      "Epoch 3/5\n",
      "15000/15000 [==============================] - 1s 70us/step - loss: 2.1905e-04 - acc: 1.0000 - val_loss: 1.0039 - val_acc: 0.8579\n",
      "Epoch 4/5\n",
      "15000/15000 [==============================] - 1s 66us/step - loss: 0.0014 - acc: 0.9998 - val_loss: 1.0349 - val_acc: 0.8579\n",
      "Epoch 5/5\n",
      "15000/15000 [==============================] - 1s 66us/step - loss: 1.3335e-04 - acc: 1.0000 - val_loss: 1.0451 - val_acc: 0.8584\n",
      "测试集的准确率为: 0.84252\n",
      "测试集的精确率为: 0.8521839269556634\n",
      "测试集的召回率为: 0.8288\n",
      "测试集的auc值为: 0.8425199999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "history = model.fit(partial_x_train,\n",
    "                   partial_y_train,\n",
    "                   epochs=5,\n",
    "                   batch_size=512,\n",
    "                   validation_data=[x_val, y_val])\n",
    "\n",
    "y_prob = model.predict(x_test)\n",
    "y_pred = y_prob\n",
    "y_pred[y_pred >= 0.5] = 1\n",
    "y_pred[y_pred < 0.5] = 0\n",
    "\n",
    "test_acc = metrics.accuracy_score(y_test, y_pred)\n",
    "test_precision = metrics.precision_score(y_test, y_pred)\n",
    "test_recall = metrics.recall_score(y_test, y_pred)\n",
    "test_auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(\"测试集的准确率为:\", test_acc)\n",
    "print(\"测试集的精确率为:\", test_precision)\n",
    "print(\"测试集的召回率为:\", test_recall)\n",
    "print(\"测试集的auc值为:\", test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 使用embedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 100\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 只使用embedding层进行训练和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 100, 64)           640000    \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               3277312   \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,987,649\n",
      "Trainable params: 3,987,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 19s 942us/step - loss: 0.4529 - acc: 0.7722 - val_loss: 0.3541 - val_acc: 0.8368\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 18s 888us/step - loss: 0.1601 - acc: 0.9397 - val_loss: 0.4860 - val_acc: 0.8152\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 18s 902us/step - loss: 0.0189 - acc: 0.9945 - val_loss: 1.1222 - val_acc: 0.8104\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 19s 932us/step - loss: 0.0079 - acc: 0.9987 - val_loss: 1.5867 - val_acc: 0.8094\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 18s 905us/step - loss: 0.0034 - acc: 0.9994 - val_loss: 2.3780 - val_acc: 0.7996\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 19s 932us/step - loss: 0.0038 - acc: 0.9996 - val_loss: 2.2691 - val_acc: 0.8158\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 18s 908us/step - loss: 0.0030 - acc: 0.9996 - val_loss: 2.4091 - val_acc: 0.8072\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 18s 921us/step - loss: 0.0017 - acc: 0.9998 - val_loss: 2.5360 - val_acc: 0.8068\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 19s 945us/step - loss: 0.0029 - acc: 0.9997 - val_loss: 2.6045 - val_acc: 0.8054\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 18s 925us/step - loss: 0.0028 - acc: 0.9997 - val_loss: 2.5872 - val_acc: 0.8052\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 64, input_length=maxlen))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "history = model.fit(x_train, y_train,\n",
    "epochs=10,\n",
    "batch_size=32,\n",
    "validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 100, 64)           640000    \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 512)               3277312   \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 3,987,649\n",
      "Trainable params: 3,987,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 19s 948us/step - loss: 0.4480 - acc: 0.7747 - val_loss: 0.3655 - val_acc: 0.8492\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 18s 912us/step - loss: 0.1597 - acc: 0.9404 - val_loss: 0.5004 - val_acc: 0.8248\n",
      "测试集的准确率为: 0.8214\n",
      "测试集的精确率为: 0.8309580690336931\n",
      "测试集的召回率为: 0.80696\n",
      "测试集的auc值为: 0.8214\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 64, input_length=maxlen))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "history = model.fit(x_train, y_train,\n",
    "epochs=2,\n",
    "batch_size=32,\n",
    "validation_split=0.2)\n",
    "\n",
    "y_prob = model.predict(x_test)\n",
    "y_pred = y_prob\n",
    "y_pred[y_pred >= 0.5] = 1\n",
    "y_pred[y_pred < 0.5] = 0\n",
    "\n",
    "test_acc = metrics.accuracy_score(y_test, y_pred)\n",
    "test_precision = metrics.precision_score(y_test, y_pred)\n",
    "test_recall = metrics.recall_score(y_test, y_pred)\n",
    "test_auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(\"测试集的准确率为:\", test_acc)\n",
    "print(\"测试集的精确率为:\", test_precision)\n",
    "print(\"测试集的召回率为:\", test_recall)\n",
    "print(\"测试集的auc值为:\", test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 使用glove预训练词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. 处理imdb原始数据和标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "imdb_dir = './data/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frank Sinatra was far from the ideal actor for westerns. He was a great actor, From Here to Eternity and The Man with The Golden arm are a proof of that, but he did not have the physique of a western hero, you identified him as an urban guy. But he tried to do his job well in Johnny Concho, the fact that the film was a failure at the box office was not his fault. I blame it on two factors: a) the story was too unusual, specially in the fact that Sinatra behaves more like a villain than as a hero throughout the movie. In a genre where people kind of expected a certain pattern, to break away from it the film has to be very good. b) the story is not convincing, it is hard to believe that a whole town will allow Sinatra to do anything he wants just because they are afraid of his brother. Also when a man shows him a special holster that will open sideways so he has not to draw the gun you wonder that if that will make him invincible, why all the gunfighters have not adopted it? I think that this film should not have been withdrawn, because any film with Sinatra is worth seeing, and in spite of its shortcomings it is still enjoyable'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n",
      "Shape of data tensor: (25000, 400)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 400\n",
    "train_samples = 20000\n",
    "validation_samples = 5000\n",
    "max_words = 5000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:train_samples]\n",
    "y_train = labels[:train_samples]\n",
    "\n",
    "x_val = data[train_samples: train_samples+validation_samples]\n",
    "y_val = labels[train_samples: train_samples+validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = './embedding/'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('belivable', 88582),\n",
       " ('macshane', 88581),\n",
       " ('cartmans', 88580),\n",
       " ('bachar', 88579),\n",
       " ('dvda', 88578),\n",
       " ('pacy', 88577),\n",
       " ('octagonal', 88576),\n",
       " ('hexagonal', 88575),\n",
       " (\"cote's\", 88574),\n",
       " ('threequels', 88573),\n",
       " ('shying', 88572),\n",
       " ('fetishwear', 88571),\n",
       " ('exhuberance', 88570),\n",
       " (\"moviegoer's\", 88569),\n",
       " ('picnicking', 88568),\n",
       " ('darwinian', 88567),\n",
       " ('blueish', 88566),\n",
       " (\"warhols'\", 88565),\n",
       " ('schreck', 88564),\n",
       " ('potee', 88563),\n",
       " ('manouvres', 88562),\n",
       " ('grandes', 88561),\n",
       " ('doozys', 88560),\n",
       " ('ascots', 88559),\n",
       " (\"d'angelo's\", 88558),\n",
       " ('megastar', 88557),\n",
       " ('waheeda', 88556),\n",
       " (\"fischer's\", 88555),\n",
       " ('choronzhon', 88554),\n",
       " ('psychomania', 88553),\n",
       " ('hypnotising', 88552),\n",
       " ('kitties', 88551),\n",
       " ('540i', 88550),\n",
       " (\"ackroyd's\", 88549),\n",
       " (\"spot'\", 88548),\n",
       " ('brimful', 88547),\n",
       " ('exporters', 88546),\n",
       " ('spasmodic', 88545),\n",
       " (\"''their\", 88544),\n",
       " (\"pair''\", 88543),\n",
       " (\"''nice\", 88542),\n",
       " (\"kieslowski's\", 88541),\n",
       " ('psychoanalyzes', 88540),\n",
       " ('outbreaking', 88539),\n",
       " ('chillness', 88538),\n",
       " (\"'evacuee'\", 88537),\n",
       " (\"'portobello\", 88536),\n",
       " ('legitimated', 88535),\n",
       " ('schema', 88534),\n",
       " ('concretely', 88533),\n",
       " ('dialectics', 88532),\n",
       " ('bergmans', 88531),\n",
       " ('stunden', 88530),\n",
       " ('siebenmal', 88529),\n",
       " ('woche', 88528),\n",
       " ('tage', 88527),\n",
       " ('sieben', 88526),\n",
       " ('schopenhauerian', 88525),\n",
       " ('unevitable', 88524),\n",
       " ('fysical', 88523),\n",
       " ('reliever', 88522),\n",
       " ('totes', 88521),\n",
       " ('liebe', 88520),\n",
       " ('coulisses', 88519),\n",
       " ('earthshaking', 88518),\n",
       " ('veneration', 88517),\n",
       " ('betrail', 88516),\n",
       " ('flowless', 88515),\n",
       " ('uncorruptable', 88514),\n",
       " ('solvent', 88513),\n",
       " ('tooltime', 88512),\n",
       " ('dorday', 88511),\n",
       " ('balme', 88510),\n",
       " ('desparate', 88509),\n",
       " ('sorbet', 88508),\n",
       " ('wiata', 88507),\n",
       " ('akerston', 88506),\n",
       " ('dunns', 88505),\n",
       " ('wellingtonian', 88504),\n",
       " ('matriach', 88503),\n",
       " ('proudest', 88502),\n",
       " ('desireless', 88501),\n",
       " ('mcgarten', 88500),\n",
       " ('polack', 88499),\n",
       " (\"mistake'\", 88498),\n",
       " ('juts', 88497),\n",
       " ('críticos', 88496),\n",
       " ('paulista', 88495),\n",
       " ('associação', 88494),\n",
       " ('monteiro', 88493),\n",
       " ('régis', 88492),\n",
       " ('cenograph', 88491),\n",
       " ('mauro', 88490),\n",
       " ('humberto', 88489),\n",
       " ('brasileiro', 88488),\n",
       " ('brazília', 88487),\n",
       " ('rodrix', 88486),\n",
       " ('zé', 88485),\n",
       " ('nélson', 88484),\n",
       " ('seboipepe', 88483),\n",
       " ('maritally', 88482),\n",
       " ('tupinambás', 88481),\n",
       " ('colassanti', 88480),\n",
       " ('arduíno', 88479),\n",
       " ('portugueses', 88478),\n",
       " ('frenches', 88477),\n",
       " ('1594', 88476),\n",
       " ('ruinous', 88475),\n",
       " (\"kantor's\", 88474),\n",
       " ('mckinley', 88473),\n",
       " ('manassas', 88472),\n",
       " ('maximillian', 88471),\n",
       " ('reunifying', 88470),\n",
       " ('miljan', 88469),\n",
       " ('wassell', 88468),\n",
       " ('dethroning', 88467),\n",
       " ('odysessy', 88466),\n",
       " ('felinni', 88465),\n",
       " ('pendulous', 88464),\n",
       " (\"'reckless'\", 88463),\n",
       " (\"'touching\", 88462),\n",
       " (\"april'\", 88461),\n",
       " (\"'enchanted\", 88460),\n",
       " (\"buccaneers'\", 88459),\n",
       " (\"'lillie'\", 88458),\n",
       " (\"annis'\", 88457),\n",
       " (\"loud'\", 88456),\n",
       " (\"'laugh\", 88455),\n",
       " ('karenina', 88454),\n",
       " (\"don't's\", 88453),\n",
       " ('briggitta', 88452),\n",
       " (\"exorcist''\", 88451),\n",
       " ('tudors', 88450),\n",
       " ('pimply', 88449),\n",
       " (\"virgin'''made\", 88448),\n",
       " ('dashiel', 88447),\n",
       " ('vraiment', 88446),\n",
       " ('feist', 88445),\n",
       " (\"twyker's\", 88444),\n",
       " ('qdlm', 88443),\n",
       " ('16ieme', 88442),\n",
       " ('resumé', 88441),\n",
       " ('roadies', 88440),\n",
       " ('spellbind', 88439),\n",
       " ('inhales', 88438),\n",
       " (\"'enchanted'\", 88437),\n",
       " ('drainage', 88436),\n",
       " ('ubernerds', 88435),\n",
       " (\"chicago's\", 88434),\n",
       " ('inquire', 88433),\n",
       " ('funnny', 88432),\n",
       " ('enoy', 88431),\n",
       " ('undeservingly', 88430),\n",
       " ('memoriam', 88429),\n",
       " ('watchword', 88428),\n",
       " ('pamanteni', 88427),\n",
       " ('dintre', 88426),\n",
       " ('iubit', 88425),\n",
       " ('cogburn', 88424),\n",
       " ('phillistines', 88423),\n",
       " (\"julien's\", 88422),\n",
       " ('tlk2', 88421),\n",
       " ('decorsia', 88420),\n",
       " ('coppery', 88419),\n",
       " (\"seaman's\", 88418),\n",
       " ('skedaddled', 88417),\n",
       " ('databases', 88416),\n",
       " ('befuddlement', 88415),\n",
       " ('chaperoning', 88414),\n",
       " ('peacekeeping', 88413),\n",
       " ('hyperbolize', 88412),\n",
       " ('vomitous', 88411),\n",
       " ('freckled', 88410),\n",
       " ('poindexter', 88409),\n",
       " ('maars', 88408),\n",
       " (\"'concider\", 88407),\n",
       " (\"'sound\", 88406),\n",
       " (\"'gigi'\", 88405),\n",
       " ('wtse', 88404),\n",
       " ('postlewaite', 88403),\n",
       " (\"coast'\", 88402),\n",
       " (\"'coast\", 88401),\n",
       " (\"playground'\", 88400),\n",
       " (\"'violent\", 88399),\n",
       " (\"interpretor'\", 88398),\n",
       " ('arsonists', 88397),\n",
       " ('fowell', 88396),\n",
       " (\"'blackboard\", 88395),\n",
       " ('target\\x97enervated', 88394),\n",
       " ('illicitly', 88393),\n",
       " ('transcribes', 88392),\n",
       " ('skullcap', 88391),\n",
       " ('klok', 88390),\n",
       " ('queensbury', 88389),\n",
       " ('haber', 88388),\n",
       " ('grinderlin', 88387),\n",
       " ('ovies', 88386),\n",
       " ('combusted', 88385),\n",
       " (\"tap's\", 88384),\n",
       " (\"nwh's\", 88383),\n",
       " (\"def's\", 88382),\n",
       " ('kermy', 88381),\n",
       " ('rockfalls', 88380),\n",
       " ('kilkenny', 88379),\n",
       " ('emeritus', 88378),\n",
       " ('epigram', 88377),\n",
       " ('bluer', 88376),\n",
       " ('missoula', 88375),\n",
       " ('reveille', 88374),\n",
       " ('sexuals', 88373),\n",
       " ('antecedents', 88372),\n",
       " (\"attached'\", 88371),\n",
       " (\"staircase'\", 88370),\n",
       " ('agrandizement', 88369),\n",
       " ('duda', 88368),\n",
       " ('gutwrenching', 88367),\n",
       " ('175', 88366),\n",
       " ('detectors', 88365),\n",
       " ('probationary', 88364),\n",
       " (\"village's\", 88363),\n",
       " ('earing', 88362),\n",
       " ('frocked', 88361),\n",
       " ('nimbly', 88360),\n",
       " ('usurper', 88359),\n",
       " (\"'sitting\", 88358),\n",
       " (\"siskel's\", 88357),\n",
       " ('chetas', 88356),\n",
       " ('antecedently', 88355),\n",
       " ('pasar', 88354),\n",
       " (\"80's'\", 88353),\n",
       " (\"nz's\", 88352),\n",
       " (\"nz'ers\", 88351),\n",
       " (\"sollett'\", 88350),\n",
       " ('kwong', 88349),\n",
       " (\"'enemies'\", 88348),\n",
       " ('ahlstedt', 88347),\n",
       " ('bjore', 88346),\n",
       " (\"'trains'\", 88345),\n",
       " ('nonviolence', 88344),\n",
       " ('embodying', 88343),\n",
       " ('makavajev', 88342),\n",
       " (\"sjoman's\", 88341),\n",
       " (\"'metafilm'\", 88340),\n",
       " ('celebre', 88339),\n",
       " ('seussical', 88338),\n",
       " ('unsolicited', 88337),\n",
       " (\"hobson's\", 88336),\n",
       " ('labirinto', 88335),\n",
       " ('ghazals', 88334),\n",
       " ('reshmmiya', 88333),\n",
       " ('himmesh', 88332),\n",
       " (\"trenholm's\", 88331),\n",
       " ('expositing', 88330),\n",
       " ('him\\x97but', 88329),\n",
       " ('ones\\x97with', 88328),\n",
       " ('trainable', 88327),\n",
       " ('elephants\\x97far', 88326),\n",
       " ('saurian', 88325),\n",
       " ('crocodiles\\x97the', 88324),\n",
       " (\"donnell's\", 88323),\n",
       " ('suberb', 88322),\n",
       " ('comraderie', 88321),\n",
       " (\"gogh'\", 88320),\n",
       " (\"'vincent\", 88319),\n",
       " (\"dufy'\", 88318),\n",
       " (\"'raoul\", 88317),\n",
       " ('dividend', 88316),\n",
       " ('kelly\\x85', 88315),\n",
       " (\"foch'\", 88314),\n",
       " (\"'nina\", 88313),\n",
       " (\"holm'\", 88312),\n",
       " (\"'celeste\", 88311),\n",
       " (\"freed's\", 88310),\n",
       " ('songbook', 88309),\n",
       " ('ontop', 88308),\n",
       " (\"girls'deaths\", 88307),\n",
       " (\"'partner'\", 88306),\n",
       " ('individuated', 88305),\n",
       " ('illuminators', 88304),\n",
       " ('numinous', 88303),\n",
       " ('metamorphically', 88302),\n",
       " (\"aidan's\", 88301),\n",
       " ('norsemen', 88300),\n",
       " ('rachford', 88299),\n",
       " ('gojn', 88298),\n",
       " ('boddhisatva', 88297),\n",
       " ('himalaya', 88296),\n",
       " ('crossings', 88295),\n",
       " ('misued', 88294),\n",
       " ('bennifer', 88293),\n",
       " ('repubhlic', 88292),\n",
       " (\"'states'\", 88291),\n",
       " (\"'provinces'\", 88290),\n",
       " ('legionairres', 88289),\n",
       " ('zfl', 88288),\n",
       " (\"oro'\", 88287),\n",
       " (\"ramon'sister\", 88286),\n",
       " ('hacienda', 88285),\n",
       " (\"'z'\", 88284),\n",
       " ('frasncisco', 88283),\n",
       " (\"'insult'\", 88282),\n",
       " (\"'council'\", 88281),\n",
       " (\"'39\", 88280),\n",
       " ('defray', 88279),\n",
       " (\"because's\", 88278),\n",
       " (\"olivia's\", 88277),\n",
       " ('fou', 88276),\n",
       " ('britan', 88275),\n",
       " ('sumptuousness', 88274),\n",
       " ('hotbeds', 88273),\n",
       " ('veche', 88272),\n",
       " ('vama', 88271),\n",
       " ('faultline', 88270),\n",
       " ('chacotero', 88269),\n",
       " (\"waissbluth's\", 88268),\n",
       " (\"'gringos'\", 88267),\n",
       " (\"santiago's\", 88266),\n",
       " ('despairingly', 88265),\n",
       " ('panamericano', 88264),\n",
       " ('protanganists', 88263),\n",
       " ('realigns', 88262),\n",
       " ('firework', 88261),\n",
       " (\"'adopts'\", 88260),\n",
       " ('padrino', 88259),\n",
       " ('conspicious', 88258),\n",
       " ('barril', 88257),\n",
       " (\"gracia's\", 88256),\n",
       " ('molden', 88255),\n",
       " ('schulmädchen', 88254),\n",
       " ('bilitis', 88253),\n",
       " ('besting', 88252),\n",
       " ('ethnographer', 88251),\n",
       " ('eastward', 88250),\n",
       " ('intrepidly', 88249),\n",
       " ('migratory', 88248),\n",
       " ('giusstissia', 88247),\n",
       " ('palassio', 88246),\n",
       " ('antevleva', 88245),\n",
       " ('unziker', 88244),\n",
       " ('conceptualized', 88243),\n",
       " ('pavlinek', 88242),\n",
       " ('orchestras', 88241),\n",
       " ('metzner', 88240),\n",
       " ('erno', 88239),\n",
       " ('smouldered', 88238),\n",
       " ('courrieres', 88237),\n",
       " ('sociable', 88236),\n",
       " ('biographies\\x97is', 88235),\n",
       " ('\\x97like', 88234),\n",
       " ('greatness\\x97and', 88233),\n",
       " ('memorialized', 88232),\n",
       " ('springs\\x97this', 88231),\n",
       " ('reactions\\x97again', 88230),\n",
       " ('soldiers\\x85simply', 88229),\n",
       " ('\\x97his', 88228),\n",
       " ('filipinos\\x97', 88227),\n",
       " ('boat\\x97thus', 88226),\n",
       " ('reactivation', 88225),\n",
       " (\"193o's\", 88224),\n",
       " ('progressive\\x97commandant', 88223),\n",
       " ('youngest\\x97and', 88222),\n",
       " (\"point's\", 88221),\n",
       " ('brigadier', 88220),\n",
       " ('point\\x97first', 88219),\n",
       " ('tribesmen\\x97thus', 88218),\n",
       " ('188o', 88217),\n",
       " ('\\x97to', 88216),\n",
       " ('ordered\\x97by', 88215),\n",
       " ('adequate\\x97and', 88214),\n",
       " ('197o', 88213),\n",
       " ('2oo5', 88212),\n",
       " ('2oo4', 88211),\n",
       " ('better\\x97than', 88210),\n",
       " ('lives\\x97for', 88209),\n",
       " ('merilu', 88208),\n",
       " ('maurren', 88207),\n",
       " ('griffit', 88206),\n",
       " ('millers', 88205),\n",
       " ('reciprocated', 88204),\n",
       " (\"sarlaac's\", 88203),\n",
       " ('sinnister', 88202),\n",
       " (\"rebellion's\", 88201),\n",
       " ('tremblay', 88200),\n",
       " ('gaz', 88199),\n",
       " ('kickass', 88198),\n",
       " (\"winners'\", 88197),\n",
       " (\"'calamity\", 88196),\n",
       " ('tooie', 88195),\n",
       " ('sm64', 88194),\n",
       " ('plat', 88193),\n",
       " (\"peach's\", 88192),\n",
       " ('beaters', 88191),\n",
       " ('cortney', 88190),\n",
       " ('hoffbrauhaus', 88189),\n",
       " ('putsch', 88188),\n",
       " ('hitlerian', 88187),\n",
       " ('winders', 88186),\n",
       " ('vendors', 88185),\n",
       " ('armistice', 88184),\n",
       " ('465', 88183),\n",
       " ('messanger', 88182),\n",
       " ('convite', 88181),\n",
       " ('anddd', 88180),\n",
       " ('gilgamesh', 88179),\n",
       " ('crimefilm', 88178),\n",
       " ('boried', 88177),\n",
       " ('watchosky', 88176),\n",
       " ('gandhis', 88175),\n",
       " ('acual', 88174),\n",
       " ('untrusting', 88173),\n",
       " ('algerians', 88172),\n",
       " (\"z's\", 88171),\n",
       " ('zhv', 88170),\n",
       " ('cinequanon', 88169),\n",
       " (\"groupe'\", 88168),\n",
       " ('woodify', 88167),\n",
       " ('unfindable', 88166),\n",
       " ('tirith', 88165),\n",
       " ('minas', 88164),\n",
       " ('numenorians', 88163),\n",
       " ('beardy', 88162),\n",
       " (\"feet'\", 88161),\n",
       " (\"'hairy\", 88160),\n",
       " (\"'cartoonish'\", 88159),\n",
       " (\"gondor's\", 88158),\n",
       " (\"'clean\", 88157),\n",
       " ('weatherworn', 88156),\n",
       " ('hollin', 88155),\n",
       " ('elven', 88154),\n",
       " ('soundscape', 88153),\n",
       " ('lothlorien', 88152),\n",
       " ('fangorn', 88151),\n",
       " (\"hobbits'\", 88150),\n",
       " (\"galadriel's\", 88149),\n",
       " ('proudfeet', 88148),\n",
       " ('rusticism', 88147),\n",
       " ('stylisation', 88146),\n",
       " ('ringwraith', 88145),\n",
       " ('sketchily', 88144),\n",
       " ('literalism', 88143),\n",
       " ('luthien', 88142),\n",
       " ('beren', 88141),\n",
       " ('glorfindel', 88140),\n",
       " ('sibley', 88139),\n",
       " (\"scorcesee's\", 88138),\n",
       " ('kil', 88137),\n",
       " ('rogge', 88136),\n",
       " ('rache', 88135),\n",
       " ('kriemhilds', 88134),\n",
       " ('not\\x85repeat', 88133),\n",
       " ('it\\x85the', 88132),\n",
       " ('film\\x85her', 88131),\n",
       " ('original\\x85but', 88130),\n",
       " (\"psycho'\\x85\", 88129),\n",
       " ('tales\\x85peter', 88128),\n",
       " ('omnibus\\x85an', 88127),\n",
       " ('hubatsek', 88126),\n",
       " ('singularity', 88125),\n",
       " (\"'westernization'\", 88124),\n",
       " ('carnivalistic', 88123),\n",
       " (\"'underground'\", 88122),\n",
       " ('upswing', 88121),\n",
       " ('stoogephiles', 88120),\n",
       " (\"achiever'\", 88119),\n",
       " (\"'quiet\", 88118),\n",
       " ('pavey', 88117),\n",
       " (\"'having\", 88116),\n",
       " ('toughen', 88115),\n",
       " (\"tough'\", 88114),\n",
       " (\"'rough\", 88113),\n",
       " ('caldicott', 88112),\n",
       " (\"'flame'\", 88111),\n",
       " ('assuaged', 88110),\n",
       " (\"dakar'\", 88109),\n",
       " ('worlde', 88108),\n",
       " ('plumpish', 88107),\n",
       " ('crinolines', 88106),\n",
       " ('currin', 88105),\n",
       " ('mcliam', 88104),\n",
       " (\"grimm's\", 88103),\n",
       " ('tieing', 88102),\n",
       " ('sodded', 88101),\n",
       " ('dyeing', 88100),\n",
       " ('rj', 88099),\n",
       " ('asheville', 88098),\n",
       " ('biltmore', 88097),\n",
       " ('pizzazz', 88096),\n",
       " ('bucketfuls', 88095),\n",
       " ('doyleluver', 88094),\n",
       " (\"lance's\", 88093),\n",
       " (\"pal'\", 88092),\n",
       " (\"'anonymous\", 88091),\n",
       " (\"cast'\", 88090),\n",
       " (\"'supporting\", 88089),\n",
       " ('recognisably', 88088),\n",
       " ('billionaires', 88087),\n",
       " ('azariah', 88086),\n",
       " ('abilityof', 88085),\n",
       " ('rabochiy', 88084),\n",
       " ('blitzkriegs', 88083),\n",
       " (\"weissberg's\", 88082),\n",
       " ('drizzled', 88081),\n",
       " ('fleadh', 88080),\n",
       " ('setembro', 88079),\n",
       " ('gital', 88078),\n",
       " ('birkina', 88077),\n",
       " ('quedraogo', 88076),\n",
       " ('alliende', 88075),\n",
       " ('allures', 88074),\n",
       " ('shiploads', 88073),\n",
       " ('rubes', 88072),\n",
       " ('enigmas', 88071),\n",
       " ('agniezska', 88070),\n",
       " ('baez', 88069),\n",
       " ('manpower', 88068),\n",
       " (\"presidente's\", 88067),\n",
       " (\"santos'\", 88066),\n",
       " ('mcbain\\x85', 88065),\n",
       " ('awstruck', 88064),\n",
       " ('peacekeepers', 88063),\n",
       " ('solyaris', 88062),\n",
       " ('perfeita', 88061),\n",
       " ('isca', 88060),\n",
       " (\"'labeled'\", 88059),\n",
       " (\"lamp's\", 88058),\n",
       " ('mcfadden', 88057),\n",
       " ('utopic', 88056),\n",
       " (\"holodeck's\", 88055),\n",
       " ('lapsing', 88054),\n",
       " ('yar', 88053),\n",
       " ('tasha', 88052),\n",
       " ('yeun', 88051),\n",
       " ('trial\\x97at', 88050),\n",
       " (\"squatters'\", 88049),\n",
       " (\"'chip'\", 88048),\n",
       " ('panged', 88047),\n",
       " ('eschelons', 88046),\n",
       " (\"'t'\", 88045),\n",
       " ('tractored', 88044),\n",
       " (\"maureen's\", 88043),\n",
       " ('cellulose', 88042),\n",
       " ('inquilino', 88041),\n",
       " ('poulange', 88040),\n",
       " ('ruas', 88039),\n",
       " ('pânico', 88038),\n",
       " (\"wodehouses'\", 88037),\n",
       " ('reardon', 88036),\n",
       " ('diapered', 88035),\n",
       " (\"amazon's\", 88034),\n",
       " ('shonen', 88033),\n",
       " ('inuyasha', 88032),\n",
       " ('handpicks', 88031),\n",
       " (\"o'donell\", 88030),\n",
       " ('grubiness', 88029),\n",
       " ('snuffing', 88028),\n",
       " ('oragami', 88027),\n",
       " ('mohican', 88026),\n",
       " ('bifocal', 88025),\n",
       " ('brenten', 88024),\n",
       " ('ozaki', 88023),\n",
       " ('127', 88022),\n",
       " (\"siv's\", 88021),\n",
       " ('repressions', 88020),\n",
       " (\"opposition'\", 88019),\n",
       " (\"'fatty'\", 88018),\n",
       " ('impersonalized', 88017),\n",
       " (\"sheba's\", 88016),\n",
       " ('hairshirts', 88015),\n",
       " ('evocatively', 88014),\n",
       " ('spotters', 88013),\n",
       " ('opaeras', 88012),\n",
       " ('fiancè', 88011),\n",
       " ('jouanneau', 88010),\n",
       " ('eugène', 88009),\n",
       " ('chevincourt', 88008),\n",
       " ('sokorowska', 88007),\n",
       " (\"'anastasia\", 88006),\n",
       " ('wahoo', 88005),\n",
       " ('nav', 88004),\n",
       " (\"montel's\", 88003),\n",
       " (\"'wants\", 88002),\n",
       " ('psp', 88001),\n",
       " ('thougths', 88000),\n",
       " ('whathaveyous', 87999),\n",
       " ('talkd', 87998),\n",
       " ('fidget', 87997),\n",
       " ('centric', 87996),\n",
       " ('unpatronising', 87995),\n",
       " ('oldsmobile', 87994),\n",
       " ('bloodedly', 87993),\n",
       " ('nineveh', 87992),\n",
       " ('obscuringly', 87991),\n",
       " ('categorically', 87990),\n",
       " ('unacceptably', 87989),\n",
       " ('elusively', 87988),\n",
       " ('ezekiel', 87987),\n",
       " ('scripturally', 87986),\n",
       " ('leniency', 87985),\n",
       " ('disprovable', 87984),\n",
       " ('cattivi', 87983),\n",
       " ('sporchi', 87982),\n",
       " ('brutti', 87981),\n",
       " ('stringy', 87980),\n",
       " ('fil', 87979),\n",
       " ('shiu', 87978),\n",
       " ('gangfights', 87977),\n",
       " (\"i'ts\", 87976),\n",
       " ('kanoodling', 87975),\n",
       " ('heyward', 87974),\n",
       " ('leland', 87973),\n",
       " ('darndest', 87972),\n",
       " ('orléans', 87971),\n",
       " ('depersonalization', 87970),\n",
       " ('ménard', 87969),\n",
       " ('ganay', 87968),\n",
       " ('léopardi', 87967),\n",
       " (\"efficiency's\", 87966),\n",
       " ('357', 87965),\n",
       " ('série', 87964),\n",
       " ('sautet', 87963),\n",
       " ('1794', 87962),\n",
       " ('aristos', 87961),\n",
       " ('20yrs', 87960),\n",
       " ('retina', 87959),\n",
       " ('naïveté', 87958),\n",
       " ('alienness', 87957),\n",
       " (\"eq'd\", 87956),\n",
       " ('unilluminated', 87955),\n",
       " (\"'dames'\", 87954),\n",
       " (\"falcon'\", 87953),\n",
       " ('medalian', 87952),\n",
       " ('talisman', 87951),\n",
       " ('weatherman', 87950),\n",
       " (\"klebb's\", 87949),\n",
       " ('goldinger', 87948),\n",
       " ('db5', 87947),\n",
       " ('mêlée', 87946),\n",
       " ('rappel', 87945),\n",
       " ('moneypenny', 87944),\n",
       " ('klebb', 87943),\n",
       " ('feirstein', 87942),\n",
       " ('kerim', 87941),\n",
       " ('lektor', 87940),\n",
       " ('octopussy', 87939),\n",
       " ('bond\\x85', 87938),\n",
       " ('soppiness', 87937),\n",
       " ('emminently', 87936),\n",
       " ('irreverant', 87935),\n",
       " ('descovered', 87934),\n",
       " ('pruner', 87933),\n",
       " ('orphanages', 87932),\n",
       " (\"roses'so\", 87931),\n",
       " (\"situation'sung\", 87930),\n",
       " (\"'reviewing\", 87929),\n",
       " ('aquires', 87928),\n",
       " (\"kapow's\", 87927),\n",
       " ('whited', 87926),\n",
       " (\"'spookiness'\", 87925),\n",
       " ('furo', 87924),\n",
       " (\"momsen's\", 87923),\n",
       " ('lizardry', 87922),\n",
       " ('outshoot', 87921),\n",
       " ('dorkily', 87920),\n",
       " ('trespasser', 87919),\n",
       " ('elsewise', 87918),\n",
       " ('cheques', 87917),\n",
       " ('purefoy', 87916),\n",
       " ('videostores', 87915),\n",
       " ('multiplied', 87914),\n",
       " (\"peet's\", 87913),\n",
       " ('albatross', 87912),\n",
       " (\"mariner's\", 87911),\n",
       " (\"walshs'\", 87910),\n",
       " ('shor', 87909),\n",
       " ('reshovsky', 87908),\n",
       " (\"waynes'\", 87907),\n",
       " ('worsel', 87906),\n",
       " ('worls', 87905),\n",
       " ('kyles', 87904),\n",
       " ('breads', 87903),\n",
       " ('ferdinandvongalitzien', 87902),\n",
       " ('galitzien', 87901),\n",
       " ('severities', 87900),\n",
       " ('leitmotiv', 87899),\n",
       " (\"graf's\", 87898),\n",
       " (\"'traveling'\", 87897),\n",
       " ('bucketful', 87896),\n",
       " ('ongoingness', 87895),\n",
       " ('dimas', 87894),\n",
       " ('evenhanded', 87893),\n",
       " ('conservativism', 87892),\n",
       " ('fairmindedness', 87891),\n",
       " ('eddington', 87890),\n",
       " (\"o'mara\", 87889),\n",
       " ('wyngarde', 87888),\n",
       " ('gaunts', 87887),\n",
       " (\"'snow'\", 87886),\n",
       " ('deputising', 87885),\n",
       " ('thanatopsis', 87884),\n",
       " (\"mencken's\", 87883),\n",
       " ('unleavened', 87882),\n",
       " ('opprobrium', 87881),\n",
       " ('hupert', 87880),\n",
       " ('dustier', 87879),\n",
       " ('lionized', 87878),\n",
       " ('denchs', 87877),\n",
       " ('viventi', 87876),\n",
       " ('morti', 87875),\n",
       " ('citta', 87874),\n",
       " ('paura', 87873),\n",
       " ('briganti', 87872),\n",
       " ('roadwarrior', 87871),\n",
       " ('abolitionism', 87870),\n",
       " ('pontente', 87869),\n",
       " ('pide', 87868),\n",
       " (\"most's\", 87867),\n",
       " ('1415', 87866),\n",
       " ('ceuta', 87865),\n",
       " ('principe', 87864),\n",
       " ('tomé', 87863),\n",
       " ('verde', 87862),\n",
       " ('guine', 87861),\n",
       " (\"vonngut's\", 87860),\n",
       " ('breakfasts', 87859),\n",
       " ('kish', 87858),\n",
       " ('dgw', 87857),\n",
       " ('snowballing', 87856),\n",
       " (\"weem's\", 87855),\n",
       " ('nuevo', 87854),\n",
       " ('quellen', 87853),\n",
       " ('somnolent', 87852),\n",
       " ('torpidly', 87851),\n",
       " ('farmzoid', 87850),\n",
       " ('lederer', 87849),\n",
       " ('membury', 87848),\n",
       " ('hampel', 87847),\n",
       " (\"kaiser's\", 87846),\n",
       " ('pone', 87845),\n",
       " ('haid', 87844),\n",
       " ('lunk', 87843),\n",
       " (\"schrieber's\", 87842),\n",
       " ('hanfstaengel', 87841),\n",
       " ('putzi', 87840),\n",
       " (\"adolph's\", 87839),\n",
       " ('pitiably', 87838),\n",
       " ('telford', 87837),\n",
       " ('brownshirt', 87836),\n",
       " ('fruttis', 87835),\n",
       " (\"hoopers'\", 87834),\n",
       " ('lisle', 87833),\n",
       " ('oafish', 87832),\n",
       " (\"bakers'\", 87831),\n",
       " (\"obers'\", 87830),\n",
       " ('arlon', 87829),\n",
       " ('psychotherapist', 87828),\n",
       " ('propositioning', 87827),\n",
       " ('collosus', 87826),\n",
       " ('ptss', 87825),\n",
       " ('t4', 87824),\n",
       " ('johansen', 87823),\n",
       " ('21849890', 87822),\n",
       " ('21849889', 87821),\n",
       " ('21849907', 87820),\n",
       " ('moldering', 87819),\n",
       " ('cruse', 87818),\n",
       " ('debitage', 87817),\n",
       " (\"guerriri's\", 87816),\n",
       " ('pueblos', 87815),\n",
       " ('sabe', 87814),\n",
       " ('quién', 87813),\n",
       " ('chuncho', 87812),\n",
       " ('manco', 87811),\n",
       " ('egoistic', 87810),\n",
       " ('guliano', 87809),\n",
       " ('storia', 87808),\n",
       " ('sporca', 87807),\n",
       " ('quella', 87806),\n",
       " ('cinecitta', 87805),\n",
       " ('fendiando', 87804),\n",
       " (\"all'italiana\", 87803),\n",
       " ('sanitizing', 87802),\n",
       " (\"india'\", 87801),\n",
       " (\"'dodgy'\", 87800),\n",
       " (\"'bombadier'\", 87799),\n",
       " (\"'rangi'\", 87798),\n",
       " (\"'sing\", 87797),\n",
       " (\"'whispering\", 87796),\n",
       " ('taverner', 87795),\n",
       " (\"interruptions'\", 87794),\n",
       " (\"'vocal\", 87793),\n",
       " ('punka', 87792),\n",
       " (\"language'\", 87791),\n",
       " (\"dah'\", 87790),\n",
       " (\"'lah\", 87789),\n",
       " (\"'lofty'\", 87788),\n",
       " ('solomons', 87787),\n",
       " ('deolali', 87786),\n",
       " ('walmington', 87785),\n",
       " (\"croft's\", 87784),\n",
       " (\"sitcoms'\", 87783),\n",
       " (\"'perry\", 87782),\n",
       " ('setna', 87781),\n",
       " ('renu', 87780),\n",
       " (\"ram'\", 87779),\n",
       " (\"'rangi\", 87778),\n",
       " (\"mum'\", 87777),\n",
       " (\"neighbour'\", 87776),\n",
       " (\"chips'\", 87775),\n",
       " (\"'curry\", 87774),\n",
       " (\"'racist'\", 87773),\n",
       " (\"'c\", 87772),\n",
       " ('shilpa', 87771),\n",
       " (\"'celebrity\", 87770),\n",
       " ('bene', 87769),\n",
       " ('nota', 87768),\n",
       " ('israelo', 87767),\n",
       " ('megalopolis', 87766),\n",
       " ('itsõ', 87765),\n",
       " ('itõs', 87764),\n",
       " (\"shushui's\", 87763),\n",
       " ('admonishes', 87762),\n",
       " ('rôyaburi', 87761),\n",
       " (\"ichi's\", 87760),\n",
       " ('gpm', 87759),\n",
       " ('gratuitus', 87758),\n",
       " ('revisitation', 87757),\n",
       " ('excell', 87756),\n",
       " ('acerbity', 87755),\n",
       " ('precociously', 87754),\n",
       " ('tantalizingly', 87753),\n",
       " ('reservedly', 87752),\n",
       " ('disdainfully', 87751),\n",
       " ('9999', 87750),\n",
       " ('whereever', 87749),\n",
       " ('contemptuously', 87748),\n",
       " (\"junge's\", 87747),\n",
       " ('nebulas', 87746),\n",
       " ('skeritt', 87745),\n",
       " ('joyrides', 87744),\n",
       " ('rollering', 87743),\n",
       " ('lyne', 87742),\n",
       " ('gun¨', 87741),\n",
       " ('¨big', 87740),\n",
       " ('posteriorly', 87739),\n",
       " ('inaugurated', 87738),\n",
       " ('obligates', 87737),\n",
       " ('hearp', 87736),\n",
       " ('carfully', 87735),\n",
       " ('woody7739', 87734),\n",
       " (\"surfing's\", 87733),\n",
       " ('hamliton', 87732),\n",
       " ('distill', 87731),\n",
       " ('tedra', 87730),\n",
       " ('loathable', 87729),\n",
       " ('desctruction', 87728),\n",
       " ('cubbi', 87727),\n",
       " ('klangs', 87726),\n",
       " ('seaduck', 87725),\n",
       " ('webby', 87724),\n",
       " ('grandmoffromero', 87723),\n",
       " ('magon', 87722),\n",
       " ('marmelstein', 87721),\n",
       " ('susi', 87720),\n",
       " ('cowles', 87719),\n",
       " ('updyke', 87718),\n",
       " (\"tried'n'true\", 87717),\n",
       " ('attainable', 87716),\n",
       " ('theorize', 87715),\n",
       " ('tlog', 87714),\n",
       " ('personnal', 87713),\n",
       " ('wierder', 87712),\n",
       " ('mclaghlan', 87711),\n",
       " ('politbiro', 87710),\n",
       " ('sssr', 87709),\n",
       " ('broz', 87708),\n",
       " ('josip', 87707),\n",
       " ('beermat', 87706),\n",
       " ('accountancy', 87705),\n",
       " ('biochemistry', 87704),\n",
       " ('chirst', 87703),\n",
       " ('unplugged', 87702),\n",
       " ('dythirambic', 87701),\n",
       " ('dupuis', 87700),\n",
       " ('signer', 87699),\n",
       " (\"hoofer's\", 87698),\n",
       " ('boulevardier', 87697),\n",
       " ('baurel', 87696),\n",
       " ('chavalier', 87695),\n",
       " (\"promotion'\", 87694),\n",
       " (\"'self\", 87693),\n",
       " ('giner', 87692),\n",
       " ('snog', 87691),\n",
       " ('dioz', 87690),\n",
       " ('commancheroes', 87689),\n",
       " ('shahan', 87688),\n",
       " ('78rpm', 87687),\n",
       " (\"panama'\", 87686),\n",
       " ('lecarre', 87685),\n",
       " (\"havana'\", 87684),\n",
       " (\"stemmin'\", 87683),\n",
       " (\"'ludere'\", 87682),\n",
       " ('illudere', 87681),\n",
       " ('authorizes', 87680),\n",
       " (\"'sad\", 87679),\n",
       " (\"'colorization'\", 87678),\n",
       " (\"'proprietary'\", 87677),\n",
       " (\"'surviving'\", 87676),\n",
       " (\"quality'\", 87675),\n",
       " (\"'guarontee'\", 87674),\n",
       " (\"'relic'\", 87673),\n",
       " (\"'standard\", 87672),\n",
       " (\"'humanity\", 87671),\n",
       " (\"'medal\", 87670),\n",
       " (\"hawking's\", 87669),\n",
       " (\"'passworthy'\", 87668),\n",
       " (\"cabal's'\", 87667),\n",
       " (\"driver'\", 87666),\n",
       " (\"'mass\", 87665),\n",
       " (\"cannon'\", 87664),\n",
       " (\"'magnetic\", 87663),\n",
       " (\"parachuting'\", 87662),\n",
       " (\"'phony\", 87661),\n",
       " (\"'experienced'\", 87660),\n",
       " ('grandparent', 87659),\n",
       " (\"lovelier'\", 87658),\n",
       " (\"matched'\", 87657),\n",
       " (\"cabal'\", 87656),\n",
       " ('giddeon', 87655),\n",
       " ('augh', 87654),\n",
       " (\"intervention'\", 87653),\n",
       " (\"'divine\", 87652),\n",
       " (\"'duality'\", 87651),\n",
       " ('manoeuvers', 87650),\n",
       " ('rêve', 87649),\n",
       " (\"netherlands's\", 87648),\n",
       " ('beserk', 87647),\n",
       " ('amercan', 87646),\n",
       " ('gharlie', 87645),\n",
       " ('futureistic', 87644),\n",
       " ('metacinema', 87643),\n",
       " ('sculpt', 87642),\n",
       " ('svankmajer', 87641),\n",
       " (\"d'adele\", 87640),\n",
       " (\"'l'histoire\", 87639),\n",
       " ('spectacled', 87638),\n",
       " (\"tenant'\", 87637),\n",
       " ('outracing', 87636),\n",
       " ('comradery', 87635),\n",
       " ('ripsnorting', 87634),\n",
       " (\"amoretti's\", 87633),\n",
       " ('indianapolis', 87632),\n",
       " ('daytona', 87631),\n",
       " ('primarilly', 87630),\n",
       " (\"japan'\", 87629),\n",
       " (\"'comics\", 87628),\n",
       " ('baloons', 87627),\n",
       " ('osterman', 87626),\n",
       " ('garcon', 87625),\n",
       " ('vieux', 87624),\n",
       " ('deacon', 87623),\n",
       " (\"blade's\", 87622),\n",
       " ('dragonflies', 87621),\n",
       " ('kerkour', 87620),\n",
       " ('appalachian', 87619),\n",
       " ('prefabricated', 87618),\n",
       " ('zafoid', 87617),\n",
       " ('magnifique', 87616),\n",
       " ('cares\\x85\\x85\\x85\\x85', 87615),\n",
       " ('cuasi', 87614),\n",
       " ('downes', 87613),\n",
       " ('encapsuling', 87612),\n",
       " ('duuum', 87611),\n",
       " ('puchase', 87610),\n",
       " (\"hall'\", 87609),\n",
       " (\"'wuthering\", 87608),\n",
       " (\"'cinderella\", 87607),\n",
       " ('appereantly', 87606),\n",
       " (\"worries'\", 87605),\n",
       " ('teenth', 87604),\n",
       " ('ump', 87603),\n",
       " (\"pinto's\", 87602),\n",
       " ('mellowed', 87601),\n",
       " ('dubliners', 87600),\n",
       " ('guineas', 87599),\n",
       " ('divorcées', 87598),\n",
       " ('pouchy', 87597),\n",
       " ('slouchy', 87596),\n",
       " ('empties', 87595),\n",
       " ('thayer', 87594),\n",
       " ('man\\x85general', 87593),\n",
       " ('northram', 87592),\n",
       " (\"wish'd\", 87591),\n",
       " (\"door's\", 87590),\n",
       " (\"'leader'\", 87589),\n",
       " (\"'fuhrer'\", 87588),\n",
       " ('unburdened', 87587),\n",
       " ('lowrie', 87586),\n",
       " ('annna', 87585),\n",
       " ('ede', 87584),\n",
       " ('enthralls', 87583),\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_index.items(), key=lambda d: d[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embeddings_vector = embeddings_index.get(word)\n",
    "        if embeddings_vector is not None:\n",
    "            embedding_matrix[i] = embeddings_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 400, 100)          500000    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 40000)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                1280032   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,780,065\n",
      "Trainable params: 1,780,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 6s 317us/step - loss: 0.7106 - acc: 0.5584 - val_loss: 0.6493 - val_acc: 0.6578\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 6s 288us/step - loss: 0.5774 - acc: 0.7086 - val_loss: 0.5879 - val_acc: 0.7026\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 6s 287us/step - loss: 0.4416 - acc: 0.7971 - val_loss: 0.5711 - val_acc: 0.7158\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 6s 284us/step - loss: 0.3619 - acc: 0.8409 - val_loss: 0.6022 - val_acc: 0.7294\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 6s 282us/step - loss: 0.3033 - acc: 0.8717 - val_loss: 0.6559 - val_acc: 0.7244\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 6s 289us/step - loss: 0.2542 - acc: 0.8918 - val_loss: 0.8837 - val_acc: 0.6942\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 6s 291us/step - loss: 0.2187 - acc: 0.9084 - val_loss: 0.7994 - val_acc: 0.7228\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 6s 287us/step - loss: 0.1851 - acc: 0.9248 - val_loss: 1.0236 - val_acc: 0.7122\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 6s 288us/step - loss: 0.1551 - acc: 0.9390 - val_loss: 0.9625 - val_acc: 0.7128\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 6s 298us/step - loss: 0.1261 - acc: 0.9515 - val_loss: 1.1155 - val_acc: 0.7156\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "                \n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 56us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.09888627307415, 0.71692]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('pre_trained_glove_model.h5')\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. TextCNN + glove预训练向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class TextCNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        # Embedding part can try multichannel as same as origin paper\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        convs = []\n",
    "        for kernel_size in [3, 4, 5]:\n",
    "            c = Conv1D(128, kernel_size, activation='relu')(embedding)\n",
    "            c = GlobalMaxPooling1D()(c)\n",
    "            convs.append(c)\n",
    "        x = Concatenate()(convs)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 400)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 400, 100)     500000      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 398, 128)     38528       embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 397, 128)     51328       embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 396, 128)     64128       embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 384)          0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1)            385         concatenate_4[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 654,369\n",
      "Trainable params: 654,369\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TextCNN(maxlen, max_words, embedding_dim).get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].set_weights([embedding_matrix])\n",
    "#model.layers[1].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 55s 3ms/step - loss: 0.3857 - acc: 0.8228 - val_loss: 0.2995 - val_acc: 0.8728\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 55s 3ms/step - loss: 0.2144 - acc: 0.9175 - val_loss: 0.2561 - val_acc: 0.8990\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 55s 3ms/step - loss: 0.1264 - acc: 0.9576 - val_loss: 0.2458 - val_acc: 0.9040\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 56s 3ms/step - loss: 0.0614 - acc: 0.9861 - val_loss: 0.2664 - val_acc: 0.9004\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 55s 3ms/step - loss: 0.0245 - acc: 0.9978 - val_loss: 0.2843 - val_acc: 0.9032\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 55s 3ms/step - loss: 0.0094 - acc: 0.9998 - val_loss: 0.3000 - val_acc: 0.9050\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 56s 3ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.3215 - val_acc: 0.9038\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 56s 3ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.3377 - val_acc: 0.9060\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 56s 3ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.3542 - val_acc: 0.9058\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 55s 3ms/step - loss: 8.2797e-04 - acc: 1.0000 - val_loss: 0.3704 - val_acc: 0.9058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feec9749780>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 18s 738us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37440894957132637, 0.90008]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 400)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 400, 100)     500000      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 398, 128)     38528       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 397, 128)     51328       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 396, 128)     64128       embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 128)          0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 128)          0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 128)          0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 384)          0           global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1)            385         concatenate_5[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 654,369\n",
      "Trainable params: 654,369\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = TextCNN(maxlen, max_words, embedding_dim).get_model()\n",
    "model2.layers[1].set_weights([embedding_matrix])\n",
    "model2.layers[1].trainable = False\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 42s 2ms/step - loss: 0.4229 - acc: 0.8029 - val_loss: 0.3200 - val_acc: 0.8660\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 37s 2ms/step - loss: 0.2734 - acc: 0.8887 - val_loss: 0.3264 - val_acc: 0.8594\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 38s 2ms/step - loss: 0.2028 - acc: 0.9222 - val_loss: 0.3090 - val_acc: 0.8762\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 38s 2ms/step - loss: 0.1373 - acc: 0.9541 - val_loss: 0.2971 - val_acc: 0.8816\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 38s 2ms/step - loss: 0.0827 - acc: 0.9813 - val_loss: 0.3194 - val_acc: 0.8714\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 38s 2ms/step - loss: 0.0536 - acc: 0.9909 - val_loss: 0.3574 - val_acc: 0.8682\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 39s 2ms/step - loss: 0.0340 - acc: 0.9955 - val_loss: 0.3385 - val_acc: 0.8856\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 38s 2ms/step - loss: 0.0151 - acc: 0.9995 - val_loss: 0.3360 - val_acc: 0.8870\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 38s 2ms/step - loss: 0.0088 - acc: 0.9999 - val_loss: 0.3573 - val_acc: 0.8856\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 38s 2ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.3639 - val_acc: 0.8880\n",
      "25000/25000 [==============================] - 18s 730us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35344335737906396, 0.88592]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model2.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "model2.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "model2.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 400)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 400, 100)     500000      input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 398, 128)     38528       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 397, 128)     51328       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 396, 128)     64128       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 128)          0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_17 (Global (None, 128)          0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_18 (Global (None, 128)          0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 384)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_max_pooling1d_17[0][0]    \n",
      "                                                                 global_max_pooling1d_18[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 1)            385         concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 654,369\n",
      "Trainable params: 654,369\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = TextCNN(maxlen, max_words, embedding_dim).get_model()\n",
    "model3.layers[1].set_weights([embedding_matrix])\n",
    "#model2.layers[1].trainable = False\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_total = np.concatenate((x_train, x_val), axis=0)\n",
    "y_total = np.concatenate((y_train, y_val), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 85s 3ms/step - loss: 0.3642 - acc: 0.8341 - val_loss: 0.3369 - val_acc: 0.8526\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 90s 4ms/step - loss: 0.2123 - acc: 0.9142 - val_loss: 0.2412 - val_acc: 0.9021\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.1225 - acc: 0.9578 - val_loss: 0.2419 - val_acc: 0.9041\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.0564 - acc: 0.9875 - val_loss: 0.2693 - val_acc: 0.9009\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 94s 4ms/step - loss: 0.0236 - acc: 0.9972 - val_loss: 0.2942 - val_acc: 0.9014\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 88s 4ms/step - loss: 0.0081 - acc: 0.9998 - val_loss: 0.3154 - val_acc: 0.9040\n",
      "25000/25000 [==============================] - 19s 775us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3154137371329218, 0.90404]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model3.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "model3.fit(x_total, y_total,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "model3.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 400)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 400, 100)     500000      input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 398, 128)     38528       embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 397, 128)     51328       embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 396, 128)     64128       embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_19 (Global (None, 128)          0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_20 (Global (None, 128)          0           conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_21 (Global (None, 128)          0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 384)          0           global_max_pooling1d_19[0][0]    \n",
      "                                                                 global_max_pooling1d_20[0][0]    \n",
      "                                                                 global_max_pooling1d_21[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1)            385         concatenate_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 654,369\n",
      "Trainable params: 654,369\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = TextCNN(maxlen, max_words, embedding_dim).get_model()\n",
    "#model4.layers[1].set_weights([embedding_matrix])\n",
    "#model2.layers[1].trainable = False\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 88s 4ms/step - loss: 0.3780 - acc: 0.8190 - val_loss: 0.2601 - val_acc: 0.8906\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.1937 - acc: 0.9267 - val_loss: 0.2470 - val_acc: 0.8983\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 91s 4ms/step - loss: 0.1017 - acc: 0.9672 - val_loss: 0.2750 - val_acc: 0.8927\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 93s 4ms/step - loss: 0.0416 - acc: 0.9905 - val_loss: 0.3153 - val_acc: 0.8936\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 92s 4ms/step - loss: 0.0131 - acc: 0.9986 - val_loss: 0.3876 - val_acc: 0.8899\n",
      "25000/25000 [==============================] - 21s 837us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3875766246571578, 0.88988]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model4.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "model4.fit(x_total, y_total,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "model4.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
