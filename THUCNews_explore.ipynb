{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THUCNews 分类预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "if sys.version_info[0] > 2:\n",
    "    is_py3 = True\n",
    "else:\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")\n",
    "    is_py3 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def native_word(word, encoding='utf-8'):\n",
    "    \"\"\"如果在python2下面使用python3训练的模型，可考虑调用此函数转化一下字符编码\"\"\"\n",
    "    if not is_py3:\n",
    "        return word.encode(encoding)\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "def native_content(content):\n",
    "    if not is_py3:\n",
    "        return content.decode('utf-8')\n",
    "    else:\n",
    "        return content\n",
    "    \n",
    "def open_file(filename, mode='r'):\n",
    "    \"\"\"\n",
    "    常用文件操作，可在python2和python3间切换.\n",
    "    mode: 'r' or 'w' for read or write\n",
    "    \"\"\"\n",
    "    if is_py3:\n",
    "        return open(filename, mode, encoding='utf-8', errors='ignore')\n",
    "    else:\n",
    "        return open(filename, mode)\n",
    "    \n",
    "def read_file(filename):\n",
    "    \"\"\"读取文件数据\"\"\"\n",
    "    contents, labels = [], []\n",
    "    with open_file(filename) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                label, content = line.strip().split('\\t')\n",
    "                if content:\n",
    "                    contents.append(list(native_content(content)))\n",
    "                    labels.append(native_content(label))\n",
    "            except:\n",
    "                pass\n",
    "    return contents, labels\n",
    "\n",
    "def build_vocab(train_dir, vocab_dir, vocab_size=5000):\n",
    "    \"\"\"根据训练集构建词汇表，存储\"\"\"\n",
    "    data_train, _ = read_file(train_dir)\n",
    "\n",
    "    all_data = []\n",
    "    for content in data_train:\n",
    "        all_data.extend(content)\n",
    "\n",
    "    counter = Counter(all_data)\n",
    "    count_pairs = counter.most_common(vocab_size - 1)\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    # 添加一个 <PAD> 来将所有文本pad为同一长度\n",
    "    words = ['<PAD>'] + list(words)\n",
    "    open_file(vocab_dir, mode='w').write('\\n'.join(words) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocab(vocab_dir):\n",
    "    \"\"\"读取词汇表\"\"\"\n",
    "    # words = open_file(vocab_dir).read().strip().split('\\n')\n",
    "    with open_file(vocab_dir) as fp:\n",
    "        # 如果是py2 则每个值都转化为unicode\n",
    "        words = [native_content(_.strip()) for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id\n",
    "\n",
    "def read_category():\n",
    "    \"\"\"读取分类目录，固定\"\"\"\n",
    "    categories = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "    categories = [native_content(x) for x in categories]\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "\n",
    "    return categories, cat_to_id\n",
    "\n",
    "def to_words(content, words):\n",
    "    \"\"\"将id表示的内容转换为文字\"\"\"\n",
    "    return ''.join(words[x] for x in content)\n",
    "\n",
    "def process_file(filename, word_to_id, cat_to_id, max_length=600):\n",
    "    \"\"\"将文件转换为id表示\"\"\"\n",
    "    contents, labels = read_file(filename)\n",
    "\n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(cat_to_id[labels[i]])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    x_pad = sequence.pad_sequences(data_id, max_length)\n",
    "    y_pad = to_categorical(label_id, num_classes=len(cat_to_id))  # 将标签转换为one-hot表示\n",
    "\n",
    "    return x_pad, y_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = './data/THUCNews/cnews.train.txt'\n",
    "valid_file = './data/THUCNews/cnews.val.txt'\n",
    "test_file = './data/THUCNews/cnews.test.txt'\n",
    "vocab_file = './data/THUCNews/cnews.vocab.txt'\n",
    "\n",
    "words, word_to_id = read_vocab(vocab_file)\n",
    "categories, cat_to_id = read_category()\n",
    "\n",
    "x_train, y_train = process_file(train_file, word_to_id, cat_to_id)\n",
    "x_val, y_val = process_file(valid_file, word_to_id, cat_to_id)\n",
    "x_test, y_test = process_file(test_file, word_to_id, cat_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 600)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TextCNN 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class TextCNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        # Embedding part can try multichannel as same as origin paper\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        convs = []\n",
    "        for kernel_size in [3, 4, 5]:\n",
    "            c = Conv1D(128, kernel_size, activation='relu')(embedding)\n",
    "            c = GlobalMaxPooling1D()(c)\n",
    "            convs.append(c)\n",
    "        x = Concatenate()(convs)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/angx/anaconda3/envs/spark_py35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/angx/anaconda3/envs/spark_py35/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 202s 4ms/step - loss: 0.0506 - acc: 0.9829 - val_loss: 0.0581 - val_acc: 0.9772\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 206s 4ms/step - loss: 0.0165 - acc: 0.9945 - val_loss: 0.0369 - val_acc: 0.9854\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 206s 4ms/step - loss: 0.0080 - acc: 0.9975 - val_loss: 0.0270 - val_acc: 0.9913\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.0255 - val_acc: 0.9921\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 211s 4ms/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.0319 - val_acc: 0.9899\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 214s 4ms/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0423 - val_acc: 0.9875\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 216s 4ms/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0382 - val_acc: 0.9897\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 600\n",
    "batch_size = 32\n",
    "embedding_dims = 100\n",
    "epochs=10\n",
    "\n",
    "model = TextCNN(maxlen, max_features, embedding_dims, class_num=10, last_activation='softmax').get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "result = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集准确率为 0.9615\n"
     ]
    }
   ],
   "source": [
    "acc = np.mean([result[i].argmax() == y_test[i].argmax() for i in range(0 , len(result))])\n",
    "print('测试集准确率为', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "50000/50000 [==============================] - 203s 4ms/step - loss: 0.0499 - acc: 0.9831 - val_loss: 0.0422 - val_acc: 0.9865\n",
      "Epoch 2/4\n",
      "50000/50000 [==============================] - 215s 4ms/step - loss: 0.0161 - acc: 0.9947 - val_loss: 0.0378 - val_acc: 0.9868\n",
      "Epoch 3/4\n",
      "50000/50000 [==============================] - 216s 4ms/step - loss: 0.0080 - acc: 0.9975 - val_loss: 0.0292 - val_acc: 0.9905\n",
      "Epoch 4/4\n",
      "50000/50000 [==============================] - 227s 5ms/step - loss: 0.0036 - acc: 0.9990 - val_loss: 0.0299 - val_acc: 0.9912\n",
      "测试集准确率为 0.9664\n"
     ]
    }
   ],
   "source": [
    "epochs=4\n",
    "\n",
    "model = TextCNN(maxlen, max_features, embedding_dims, class_num=10, last_activation='softmax').get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "result = model.predict(x_test)\n",
    "acc = np.mean([result[i].argmax() == y_test[i].argmax() for i in range(0 , len(result))])\n",
    "print('测试集准确率为', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TextRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "class TextRNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = LSTM(128)(embedding)  # LSTM or GRU\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 456s 9ms/step - loss: 0.1731 - acc: 0.9368 - val_loss: 0.2113 - val_acc: 0.9229\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 454s 9ms/step - loss: 0.1408 - acc: 0.9500 - val_loss: 0.2036 - val_acc: 0.9321\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 455s 9ms/step - loss: 0.1471 - acc: 0.9479 - val_loss: 0.1553 - val_acc: 0.9451\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 471s 9ms/step - loss: 0.1021 - acc: 0.9648 - val_loss: 0.0998 - val_acc: 0.9684\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 453s 9ms/step - loss: 0.0540 - acc: 0.9823 - val_loss: 0.0904 - val_acc: 0.9717\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 469s 9ms/step - loss: 0.0487 - acc: 0.9839 - val_loss: 0.0754 - val_acc: 0.9764\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 432s 9ms/step - loss: 0.0337 - acc: 0.9891 - val_loss: 0.0698 - val_acc: 0.9776\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 402s 8ms/step - loss: 0.0253 - acc: 0.9918 - val_loss: 0.0589 - val_acc: 0.9817\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 403s 8ms/step - loss: 0.0203 - acc: 0.9935 - val_loss: 0.0595 - val_acc: 0.9822\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 411s 8ms/step - loss: 0.0173 - acc: 0.9945 - val_loss: 0.0573 - val_acc: 0.9832\n",
      "测试集准确率为 0.9399\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 600\n",
    "batch_size = 32\n",
    "embedding_dims = 100\n",
    "epochs=10\n",
    "\n",
    "model = TextRNN(maxlen, max_features, embedding_dims, class_num=10, last_activation='softmax').get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_val, y_val))\n",
    "\n",
    "result = model.predict(x_test)\n",
    "acc = np.mean([result[i].argmax() == y_test[i].argmax() for i in range(0 , len(result))])\n",
    "print('测试集准确率为', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. RCNN (BiLstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding, Dense, Lambda, Concatenate, Conv1D, GlobalMaxPooling1D, LSTM, Bidirectional\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class RCNN_Att_BiLstm(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input_current = Input((self.maxlen,))\n",
    "        input_left = Input((self.maxlen,))\n",
    "        input_right = Input((self.maxlen,))\n",
    "\n",
    "        embedder = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
    "        embedding_current = embedder(input_current)\n",
    "        embedding_left = embedder(input_left)\n",
    "        embedding_right = embedder(input_right)\n",
    "    \n",
    "        x_left = Bidirectional(LSTM(128, return_sequences=True))(embedding_left)\n",
    "        x_right = Bidirectional(LSTM(128, return_sequences=True, go_backwards=True))(embedding_right)\n",
    "        x_right = Lambda(lambda x: K.reverse(x, axes=1))(x_right)\n",
    "        x = Concatenate(axis=2)([x_left, embedding_current, x_right])\n",
    "\n",
    "        x = Conv1D(64, kernel_size=1, activation='tanh')(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=[input_current, input_left, input_right], outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 1024s 20ms/step - loss: 0.0642 - acc: 0.9781 - val_loss: 0.0603 - val_acc: 0.9801\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 1056s 21ms/step - loss: 0.0212 - acc: 0.9929 - val_loss: 0.0423 - val_acc: 0.9864\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 1043s 21ms/step - loss: 0.0145 - acc: 0.9953 - val_loss: 0.0432 - val_acc: 0.9860\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 1064s 21ms/step - loss: 0.0100 - acc: 0.9967 - val_loss: 0.0475 - val_acc: 0.9833\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 1077s 22ms/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0441 - val_acc: 0.9865\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 600\n",
    "batch_size = 32\n",
    "embedding_dims = 100\n",
    "epochs=5\n",
    "\n",
    "x_train_current = x_train\n",
    "x_train_left = np.hstack([np.expand_dims(x_train[:, 0], axis=1), x_train[:, 0:-1]])\n",
    "x_train_right = np.hstack([x_train[:, 1:], np.expand_dims(x_train[:, -1], axis=1)])\n",
    "x_val_current = x_val\n",
    "x_val_left = np.hstack([np.expand_dims(x_val[:, 0], axis=1), x_val[:, 0:-1]])\n",
    "x_val_right = np.hstack([x_val[:, 1:], np.expand_dims(x_val[:, -1], axis=1)])\n",
    "x_test_current = x_test\n",
    "x_test_left = np.hstack([np.expand_dims(x_test[:, 0], axis=1), x_test[:, 0:-1]])\n",
    "x_test_right = np.hstack([x_test[:, 1:], np.expand_dims(x_test[:, -1], axis=1)])\n",
    "\n",
    "model = RCNN_Att_BiLstm(maxlen, max_features, embedding_dims, class_num=10, last_activation='softmax').get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit([x_train_left, x_train_current, x_train_right], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=([x_val_current, x_val_left, x_val_right], y_val))\n",
    "\n",
    "result = model.predict([x_test_current, x_test_left, x_test_right])\n",
    "acc = np.mean([result[i].argmax() == y_test[i].argmax() for i in range(0 , len(result))])\n",
    "print('测试集准确率为', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
