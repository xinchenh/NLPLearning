{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class TextCNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        # Embedding part can try multichannel as same as origin paper\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        convs = []\n",
    "        for kernel_size in [3, 4, 5]:\n",
    "            c = Conv1D(128, kernel_size, activation='relu')(embedding)\n",
    "            c = GlobalMaxPooling1D()(c)\n",
    "            convs.append(c)\n",
    "        x = Concatenate()(convs)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "WARNING:tensorflow:From /home/angx/anaconda3/envs/spark_py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train...\n",
      "WARNING:tensorflow:From /home/angx/anaconda3/envs/spark_py35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/angx/anaconda3/envs/spark_py35/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.3916 - acc: 0.8142 - val_loss: 0.3195 - val_acc: 0.8627\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.2114 - acc: 0.9160 - val_loss: 0.2531 - val_acc: 0.8962\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.1331 - acc: 0.9524 - val_loss: 0.2729 - val_acc: 0.8920\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 48s 2ms/step - loss: 0.0758 - acc: 0.9768 - val_loss: 0.3136 - val_acc: 0.8871\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 0.0351 - acc: 0.9924 - val_loss: 0.3732 - val_acc: 0.8872\n",
      "Test data accuracy is  0.88724\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = TextCNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "class TextRNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = LSTM(128)(embedding)  # LSTM or GRU\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 147s 6ms/step - loss: 0.5209 - acc: 0.7399 - val_loss: 0.6029 - val_acc: 0.6649\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 159s 6ms/step - loss: 0.3440 - acc: 0.8560 - val_loss: 0.3223 - val_acc: 0.8609\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 156s 6ms/step - loss: 0.2670 - acc: 0.8942 - val_loss: 0.3262 - val_acc: 0.8650\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 152s 6ms/step - loss: 0.2238 - acc: 0.9134 - val_loss: 0.3200 - val_acc: 0.8655\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 154s 6ms/step - loss: 0.1927 - acc: 0.9266 - val_loss: 0.3469 - val_acc: 0.8701\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 153s 6ms/step - loss: 0.1725 - acc: 0.9334 - val_loss: 0.3965 - val_acc: 0.8710\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.1373 - acc: 0.9493 - val_loss: 0.3988 - val_acc: 0.8573\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 153s 6ms/step - loss: 0.1366 - acc: 0.9482 - val_loss: 0.4446 - val_acc: 0.8590\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 152s 6ms/step - loss: 0.1133 - acc: 0.9575 - val_loss: 0.4195 - val_acc: 0.8686\n",
      "Test...\n",
      "Test data accuracy is  0.86864\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = TextRNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiTextRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class BiTextRNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = Bidirectional(LSTM(128))(embedding)  # LSTM or GRU\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 184s 7ms/step - loss: 0.6075 - acc: 0.6719 - val_loss: 0.6527 - val_acc: 0.6428\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 200s 8ms/step - loss: 0.4184 - acc: 0.8110 - val_loss: 0.3754 - val_acc: 0.8446\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 208s 8ms/step - loss: 0.3027 - acc: 0.8764 - val_loss: 0.3092 - val_acc: 0.8692\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 207s 8ms/step - loss: 0.2554 - acc: 0.8986 - val_loss: 0.3063 - val_acc: 0.8773\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 203s 8ms/step - loss: 0.2291 - acc: 0.9104 - val_loss: 0.2958 - val_acc: 0.8784\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 202s 8ms/step - loss: 0.1976 - acc: 0.9249 - val_loss: 0.3111 - val_acc: 0.8690\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 199s 8ms/step - loss: 0.1767 - acc: 0.9323 - val_loss: 0.3274 - val_acc: 0.8691\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 202s 8ms/step - loss: 0.1536 - acc: 0.9432 - val_loss: 0.3592 - val_acc: 0.8740\n",
      "Test...\n",
      "Test data accuracy is  0.87396\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = BiTextRNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class TextAttentionBiRNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True))(embedding)  # LSTM or GRU\n",
    "        x = Attention(self.maxlen)(x)\n",
    "        \n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 206s 8ms/step - loss: 0.4416 - acc: 0.7856 - val_loss: 0.3534 - val_acc: 0.8529\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 219s 9ms/step - loss: 0.2696 - acc: 0.8908 - val_loss: 0.2771 - val_acc: 0.8864\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 228s 9ms/step - loss: 0.2068 - acc: 0.9189 - val_loss: 0.2788 - val_acc: 0.8844\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 228s 9ms/step - loss: 0.1724 - acc: 0.9356 - val_loss: 0.2785 - val_acc: 0.8834\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 219s 9ms/step - loss: 0.1408 - acc: 0.9486 - val_loss: 0.2928 - val_acc: 0.8850\n",
      "Test...\n",
      "Test data accuracy is  0.88496\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = TextAttentionBiRNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding, Dense, SimpleRNN, Lambda, Concatenate, Conv1D, GlobalMaxPooling1D, LSTM\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "class RCNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input_current = Input((self.maxlen,))\n",
    "        input_left = Input((self.maxlen,))\n",
    "        input_right = Input((self.maxlen,))\n",
    "\n",
    "        embedder = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
    "        embedding_current = embedder(input_current)\n",
    "        embedding_left = embedder(input_left)\n",
    "        embedding_right = embedder(input_right)\n",
    "\n",
    "        x_left = LSTM(128, return_sequences=True)(embedding_left)\n",
    "        x_right = LSTM(128, return_sequences=True, go_backwards=True)(embedding_right)\n",
    "        x_right = Lambda(lambda x: K.reverse(x, axes=1))(x_right)\n",
    "        x = Concatenate(axis=2)([x_left, embedding_current, x_right])\n",
    "\n",
    "        x = Conv1D(64, kernel_size=1, activation='tanh')(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=[input_current, input_left, input_right], outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Prepare input for model...\n",
      "x_train_current shape: (25000, 400)\n",
      "x_train_left shape: (25000, 400)\n",
      "x_train_right shape: (25000, 400)\n",
      "x_test_current shape: (25000, 400)\n",
      "x_test_left shape: (25000, 400)\n",
      "x_test_right shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 224s 9ms/step - loss: 0.4024 - acc: 0.8178 - val_loss: 0.2848 - val_acc: 0.8806\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 229s 9ms/step - loss: 0.2363 - acc: 0.9056 - val_loss: 0.2576 - val_acc: 0.8965\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 228s 9ms/step - loss: 0.1699 - acc: 0.9348 - val_loss: 0.2542 - val_acc: 0.8980\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 232s 9ms/step - loss: 0.1238 - acc: 0.9558 - val_loss: 0.2962 - val_acc: 0.8894\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 235s 9ms/step - loss: 0.0774 - acc: 0.9744 - val_loss: 0.3308 - val_acc: 0.8894\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 227s 9ms/step - loss: 0.0459 - acc: 0.9851 - val_loss: 0.4009 - val_acc: 0.8849\n",
      "Test...\n",
      "Test data accuracy is  0.88492\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Prepare input for model...')\n",
    "x_train_current = x_train\n",
    "x_train_left = np.hstack([np.expand_dims(x_train[:, 0], axis=1), x_train[:, 0:-1]])\n",
    "x_train_right = np.hstack([x_train[:, 1:], np.expand_dims(x_train[:, -1], axis=1)])\n",
    "x_test_current = x_test\n",
    "x_test_left = np.hstack([np.expand_dims(x_test[:, 0], axis=1), x_test[:, 0:-1]])\n",
    "x_test_right = np.hstack([x_test[:, 1:], np.expand_dims(x_test[:, -1], axis=1)])\n",
    "print('x_train_current shape:', x_train_current.shape)\n",
    "print('x_train_left shape:', x_train_left.shape)\n",
    "print('x_train_right shape:', x_train_right.shape)\n",
    "print('x_test_current shape:', x_test_current.shape)\n",
    "print('x_test_left shape:', x_test_left.shape)\n",
    "print('x_test_right shape:', x_test_right.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = RCNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit([x_train_current, x_train_left, x_train_right], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=([x_test_current, x_test_left, x_test_right], y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict([x_test_current, x_test_left, x_test_right])\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCNN (Attention + BiLstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding, Dense, SimpleRNN, Lambda, Concatenate, Conv1D, GlobalMaxPooling1D, LSTM\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "\n",
    "\n",
    "class RCNN_Att_BiLstm(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input_current = Input((self.maxlen,))\n",
    "        input_left = Input((self.maxlen,))\n",
    "        input_right = Input((self.maxlen,))\n",
    "\n",
    "        embedder = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
    "        embedding_current = embedder(input_current)\n",
    "        embedding_left = embedder(input_left)\n",
    "        embedding_right = embedder(input_right)\n",
    "        \n",
    "        x_left = Bidirectional(LSTM(128, return_sequences=True))(embedding_left)  # LSTM or GRU\n",
    "        x_left = Attention(self.maxlen)(x_left)\n",
    "        \n",
    "        x_right = Bidirectional(LSTM(128, return_sequences=True))(embedding_right)  # LSTM or GRU\n",
    "        x_right = Attention(self.maxlen)(x_right)\n",
    "\n",
    "        #x_left = LSTM(128, return_sequences=True)(embedding_left)\n",
    "        #x_right = LSTM(128, return_sequences=True, go_backwards=True)(embedding_right)\n",
    "        x_right = Lambda(lambda x: K.reverse(x, axes=1))(x_right)\n",
    "        \n",
    "        x = Concatenate(axis=2)([x_left, embedding_current, x_right])\n",
    "\n",
    "        x = Conv1D(64, kernel_size=1, activation='tanh')(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=[input_current, input_left, input_right], outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Prepare input for model...\n",
      "x_train_current shape: (25000, 400)\n",
      "x_train_left shape: (25000, 400)\n",
      "x_train_right shape: (25000, 400)\n",
      "x_test_current shape: (25000, 400)\n",
      "x_test_left shape: (25000, 400)\n",
      "x_test_right shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 222s 9ms/step - loss: 0.3931 - acc: 0.8187 - val_loss: 0.2986 - val_acc: 0.8729\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 230s 9ms/step - loss: 0.2336 - acc: 0.9054 - val_loss: 0.2669 - val_acc: 0.8896\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 232s 9ms/step - loss: 0.1691 - acc: 0.9364 - val_loss: 0.2684 - val_acc: 0.8916\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 237s 9ms/step - loss: 0.1163 - acc: 0.9596 - val_loss: 0.3161 - val_acc: 0.8829\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 226s 9ms/step - loss: 0.0761 - acc: 0.9730 - val_loss: 0.3681 - val_acc: 0.8804\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 227s 9ms/step - loss: 0.0472 - acc: 0.9839 - val_loss: 0.4083 - val_acc: 0.8818\n",
      "Test...\n",
      "Test data accuracy is  0.8818\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Prepare input for model...')\n",
    "x_train_current = x_train\n",
    "x_train_left = np.hstack([np.expand_dims(x_train[:, 0], axis=1), x_train[:, 0:-1]])\n",
    "x_train_right = np.hstack([x_train[:, 1:], np.expand_dims(x_train[:, -1], axis=1)])\n",
    "x_test_current = x_test\n",
    "x_test_left = np.hstack([np.expand_dims(x_test[:, 0], axis=1), x_test[:, 0:-1]])\n",
    "x_test_right = np.hstack([x_test[:, 1:], np.expand_dims(x_test[:, -1], axis=1)])\n",
    "print('x_train_current shape:', x_train_current.shape)\n",
    "print('x_train_left shape:', x_train_left.shape)\n",
    "print('x_train_right shape:', x_train_right.shape)\n",
    "print('x_test_current shape:', x_test_current.shape)\n",
    "print('x_test_left shape:', x_test_left.shape)\n",
    "print('x_test_right shape:', x_test_right.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = RCNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit([x_train_current, x_train_left, x_train_right], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=([x_test_current, x_test_left, x_test_right], y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict([x_test_current, x_test_left, x_test_right])\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCNNVariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Concatenate, Conv1D, Bidirectional, LSTM, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "class RCNNVariant(object):\n",
    "    \"\"\"Variant of RCNN.\n",
    "        Base on structure of RCNN, we do some improvement:\n",
    "        1. Ignore the shift for left/right context.\n",
    "        2. Use Bidirectional LSTM/GRU to encode context.\n",
    "        3. Use Multi-CNN to represent the semantic vectors.\n",
    "        4. Use ReLU instead of Tanh.\n",
    "        5. Use both AveragePooling and MaxPooling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "\n",
    "        x_context = Bidirectional(LSTM(128, return_sequences=True))(embedding)\n",
    "        x = Concatenate()([embedding, x_context])\n",
    "\n",
    "        convs = []\n",
    "        for kernel_size in range(1, 5):\n",
    "            conv = Conv1D(128, kernel_size, activation='relu')(x)\n",
    "            convs.append(conv)\n",
    "        poolings = [GlobalAveragePooling1D()(conv) for conv in convs] + [GlobalMaxPooling1D()(conv) for conv in convs]\n",
    "        x = Concatenate()(poolings)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 389s 16ms/step - loss: 0.3569 - acc: 0.8370 - val_loss: 0.2580 - val_acc: 0.8932\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 397s 16ms/step - loss: 0.2030 - acc: 0.9220 - val_loss: 0.2470 - val_acc: 0.8983\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 385s 15ms/step - loss: 0.1520 - acc: 0.9412 - val_loss: 0.2626 - val_acc: 0.8954\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 389s 16ms/step - loss: 0.1014 - acc: 0.9647 - val_loss: 0.3091 - val_acc: 0.8868\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 386s 15ms/step - loss: 0.0616 - acc: 0.9792 - val_loss: 0.4233 - val_acc: 0.8780\n",
      "Test...\n",
      "Test data accuracy is  0.87796\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = RCNNVariant(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Bidirectional, LSTM, TimeDistributed\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "    \n",
    "class HAN(object):\n",
    "    def __init__(self, maxlen_sentence, maxlen_word, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen_sentence = maxlen_sentence\n",
    "        self.maxlen_word = maxlen_word\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        # Word part\n",
    "        input_word = Input(shape=(self.maxlen_word,))\n",
    "        x_word = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen_word)(input_word)\n",
    "        x_word = Bidirectional(LSTM(128, return_sequences=True))(x_word)  # LSTM or GRU\n",
    "        x_word = Attention(self.maxlen_word)(x_word)\n",
    "        model_word = Model(input_word, x_word)\n",
    "\n",
    "        # Sentence part\n",
    "        input = Input(shape=(self.maxlen_sentence, self.maxlen_word))\n",
    "        x_sentence = TimeDistributed(model_word)(input)\n",
    "        x_sentence = Bidirectional(LSTM(128, return_sequences=True))(x_sentence)  # LSTM or GRU\n",
    "        x_sentence = Attention(self.maxlen_sentence)(x_sentence)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x_sentence)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x #sentence x #word)...\n",
      "x_train shape: (25000, 16, 25)\n",
      "x_test shape: (25000, 16, 25)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.3617 - acc: 0.8330 - val_loss: 0.3229 - val_acc: 0.8607\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 164s 7ms/step - loss: 0.2338 - acc: 0.9078 - val_loss: 0.2705 - val_acc: 0.8865\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 172s 7ms/step - loss: 0.1882 - acc: 0.9293 - val_loss: 0.2873 - val_acc: 0.8844\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 171s 7ms/step - loss: 0.1501 - acc: 0.9457 - val_loss: 0.2986 - val_acc: 0.8859\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 171s 7ms/step - loss: 0.1141 - acc: 0.9608 - val_loss: 0.3329 - val_acc: 0.8839\n",
      "Test...\n",
      "Test data accuracy is  0.88392\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen_sentence = 16\n",
    "maxlen_word = 25\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x #sentence x #word)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen_sentence * maxlen_word)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen_sentence * maxlen_word)\n",
    "x_train = x_train.reshape((len(x_train), maxlen_sentence, maxlen_word))\n",
    "x_test = x_test.reshape((len(x_test), maxlen_sentence, maxlen_word))\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = HAN(maxlen_sentence, maxlen_word, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "class FastText(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = GlobalAveragePooling1D()(embedding)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Average train sequence length: 238\n",
      "Average test sequence length: 230\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/50\n",
      "25000/25000 [==============================] - 4s 141us/step - loss: 0.6170 - acc: 0.7280 - val_loss: 0.5125 - val_acc: 0.8145\n",
      "Epoch 2/50\n",
      "25000/25000 [==============================] - 3s 115us/step - loss: 0.4228 - acc: 0.8535 - val_loss: 0.3802 - val_acc: 0.8615\n",
      "Epoch 3/50\n",
      "25000/25000 [==============================] - 3s 116us/step - loss: 0.3311 - acc: 0.8796 - val_loss: 0.3284 - val_acc: 0.8745\n",
      "Epoch 4/50\n",
      "25000/25000 [==============================] - 3s 118us/step - loss: 0.2885 - acc: 0.8924 - val_loss: 0.3042 - val_acc: 0.8827\n",
      "Epoch 5/50\n",
      "25000/25000 [==============================] - 3s 115us/step - loss: 0.2621 - acc: 0.9028 - val_loss: 0.2909 - val_acc: 0.8844\n",
      "Epoch 6/50\n",
      "25000/25000 [==============================] - 3s 115us/step - loss: 0.2441 - acc: 0.9096 - val_loss: 0.2858 - val_acc: 0.8858\n",
      "Epoch 7/50\n",
      "25000/25000 [==============================] - 3s 117us/step - loss: 0.2292 - acc: 0.9144 - val_loss: 0.2814 - val_acc: 0.8869\n",
      "Epoch 8/50\n",
      "25000/25000 [==============================] - 3s 118us/step - loss: 0.2179 - acc: 0.9189 - val_loss: 0.2798 - val_acc: 0.8878\n",
      "Epoch 9/50\n",
      "25000/25000 [==============================] - 3s 121us/step - loss: 0.2085 - acc: 0.9227 - val_loss: 0.2839 - val_acc: 0.8854\n",
      "Epoch 10/50\n",
      "25000/25000 [==============================] - 3s 118us/step - loss: 0.2010 - acc: 0.9266 - val_loss: 0.2830 - val_acc: 0.8866\n",
      "Epoch 11/50\n",
      "25000/25000 [==============================] - 3s 116us/step - loss: 0.1941 - acc: 0.9292 - val_loss: 0.2860 - val_acc: 0.8860\n",
      "Test...\n",
      "Test data accuracy is  0.886\n"
     ]
    }
   ],
   "source": [
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    # >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    # >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "\n",
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 1\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 50\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = FastText(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import jieba\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def preprocessing(data):\n",
    "    stopword_list = [k.strip() for k in open('./data/stop_words.utf8', encoding='utf8').readlines() if k.strip() != '']\n",
    "    # å¤çdata\n",
    "    cutWords_data = []\n",
    "    for article in data['body'].astype(str):\n",
    "        cutWords = [k for k in jieba.cut(article) if k not in stopword_list]\n",
    "        cutWords_data.append(cutWords)\n",
    "    tokenizer = Tokenizer(num_words=1000)  # å»ºç«ä¸ä¸ª2000ä¸ªåè¯çå­å¸\n",
    "    tokenizer.fit_on_texts(cutWords_data)\n",
    "    # å¯¹æ¯ä¸ç¯æç« è½¬æ¢ä¸ºæ°å­åè¡¨ï¼ä½¿ç¨æ¯ä¸ªè¯çç¼å·è¿è¡ç¼å·\n",
    "    x_data_seq = tokenizer.texts_to_sequences(cutWords_data)\n",
    "    x_data = sequence.pad_sequences(x_data_seq, maxlen=400)\n",
    "    y_data = np.array(data['label'].values)\n",
    "\n",
    "    x_train, x_test, y_train,  y_test = train_test_split(x_data, y_data,test_size=0.2, random_state=42)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ°´ææ æ³¨é¢æµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/water_artical.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a.id</th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "      <th>imgnum</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ffbe23538f7c9c15b1fb3bce338d39f1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://renderh5ui.inveno.com/detail.html?app=...</td>\n",
       "      <td>0</td>\n",
       "      <td>âæäºçå¥³å­©é½ä¸è¦å½©ç¤¼ââå©å©ï¼é£ä¸æ¯æäºï¼é£æ¯æ²¡èå­â</td>\n",
       "      <td>ç¨çè¯ææçè¯è¯­ï¼è®²è¿°å³äºå»ççå°ç¥è¯ï¼å¤§å®¶å¥½ï¼è¿éæ¯å»çå°åå£«ï¼æè°¢å¤§å®¶çè§çï¼å¸æå¤§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86990225caf5829bc4b5f7b145a7e2ac</td>\n",
       "      <td>0</td>\n",
       "      <td>https://youliao.163yun.com/api-server/rss/xiao...</td>\n",
       "      <td>0</td>\n",
       "      <td>é»æ¯æ¸æå´æ°¸åå è¢«è°æ¥çç½ªèªæï¼å´æ°¸åææ°ååºè¯æèªå·±å®å¨</td>\n",
       "      <td>3æ1å·åæ¨ï¼é»æ¯æ¸çªç¶å¨ç¤¾äº¤å¹³å°ä¸­åæç§°âå´æ°¸åèªæäºâï¼å¼èµ·èè®ºä¸çåç¶ï¼ä¸ç¥éæ¯å¦ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fed8e7c9424115e07ce4860848440582</td>\n",
       "      <td>0</td>\n",
       "      <td>https://youliao.163yun.com/api-server/rss/xiao...</td>\n",
       "      <td>0</td>\n",
       "      <td>ç·å­å11ç½è´­äºä¸ç®±æ¹ä¾¿é¢ï¼æ¶å°å¿«éåï¼æå¼åè£¹ç´æ¥åäº</td>\n",
       "      <td>åä½ç½åå¤§å®¶å¥½ï¼æè°çæ´»ç¬æ¯ä¸åï¼ä»å¤©çæä»¬æ°¸è¿æ æ³é¢æµæå¤©ä¼åçä»ä¹ï¼ä¸çä¹å¤§æ¯å¤©é½ä¼æä¸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f8911a9e254d4fc2813d578ad8cd1cef</td>\n",
       "      <td>1</td>\n",
       "      <td>https://renderh5ui.inveno.com/detail.html?app=...</td>\n",
       "      <td>0</td>\n",
       "      <td>èå©æ¥æäºå²æåç«ï¼æå¯¹å¥¹è¯´äºä¸å¥è¯ï¼ç¬¬äºå¤©å°å§¨å­ä¸é¨æ¥éªæ</td>\n",
       "      <td>ç°å¨å¥³äººçæ¯å¾éº»ç¦ï¼å¶å®æä»¬ç·äººå¾æ¸æ¥ï¼å¥¹ä»¬å¥³äººæ¯ä¸ä¸ªææ¥è¿ç§äºæå¾éº»ç¦ï¼ä½æ¯...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9c4bc77498ec876614cb47bfd8edb978</td>\n",
       "      <td>0</td>\n",
       "      <td>https://xw.qq.com/partner/mibrowser/20190118A0...</td>\n",
       "      <td>3</td>\n",
       "      <td>æå¥é£æ³¢æªå¹³ï¼æ éæåè¢«æ¨ä¸é£å£æµªå°</td>\n",
       "      <td>2019å¹´1æ1æ¥ï¼å¤©æ´¥å¸å¬å®æºå³å¯¹æå¥å¬å¸æ¶å«ç»ç»ãé¢å¯¼ä¼ éæ´»å¨ç½ªåèåå¹¿åç½ªç«æ¡ä¾¦æ¥ãæ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               a.id  label  \\\n",
       "0  ffbe23538f7c9c15b1fb3bce338d39f1      1   \n",
       "1  86990225caf5829bc4b5f7b145a7e2ac      0   \n",
       "2  fed8e7c9424115e07ce4860848440582      0   \n",
       "3  f8911a9e254d4fc2813d578ad8cd1cef      1   \n",
       "4  9c4bc77498ec876614cb47bfd8edb978      0   \n",
       "\n",
       "                                                 url  imgnum  \\\n",
       "0  https://renderh5ui.inveno.com/detail.html?app=...       0   \n",
       "1  https://youliao.163yun.com/api-server/rss/xiao...       0   \n",
       "2  https://youliao.163yun.com/api-server/rss/xiao...       0   \n",
       "3  https://renderh5ui.inveno.com/detail.html?app=...       0   \n",
       "4  https://xw.qq.com/partner/mibrowser/20190118A0...       3   \n",
       "\n",
       "                            title  \\\n",
       "0    âæäºçå¥³å­©é½ä¸è¦å½©ç¤¼ââå©å©ï¼é£ä¸æ¯æäºï¼é£æ¯æ²¡èå­â   \n",
       "1   é»æ¯æ¸æå´æ°¸åå è¢«è°æ¥çç½ªèªæï¼å´æ°¸åææ°ååºè¯æèªå·±å®å¨   \n",
       "2    ç·å­å11ç½è´­äºä¸ç®±æ¹ä¾¿é¢ï¼æ¶å°å¿«éåï¼æå¼åè£¹ç´æ¥åäº   \n",
       "3  èå©æ¥æäºå²æåç«ï¼æå¯¹å¥¹è¯´äºä¸å¥è¯ï¼ç¬¬äºå¤©å°å§¨å­ä¸é¨æ¥éªæ   \n",
       "4              æå¥é£æ³¢æªå¹³ï¼æ éæåè¢«æ¨ä¸é£å£æµªå°   \n",
       "\n",
       "                                                body  \n",
       "0    ç¨çè¯ææçè¯è¯­ï¼è®²è¿°å³äºå»ççå°ç¥è¯ï¼å¤§å®¶å¥½ï¼è¿éæ¯å»çå°åå£«ï¼æè°¢å¤§å®¶çè§çï¼å¸æå¤§...  \n",
       "1   3æ1å·åæ¨ï¼é»æ¯æ¸çªç¶å¨ç¤¾äº¤å¹³å°ä¸­åæç§°âå´æ°¸åèªæäºâï¼å¼èµ·èè®ºä¸çåç¶ï¼ä¸ç¥éæ¯å¦ç...  \n",
       "2  åä½ç½åå¤§å®¶å¥½ï¼æè°çæ´»ç¬æ¯ä¸åï¼ä»å¤©çæä»¬æ°¸è¿æ æ³é¢æµæå¤©ä¼åçä»ä¹ï¼ä¸çä¹å¤§æ¯å¤©é½ä¼æä¸...  \n",
       "3         ç°å¨å¥³äººçæ¯å¾éº»ç¦ï¼å¶å®æä»¬ç·äººå¾æ¸æ¥ï¼å¥¹ä»¬å¥³äººæ¯ä¸ä¸ªææ¥è¿ç§äºæå¾éº»ç¦ï¼ä½æ¯...  \n",
       "4  2019å¹´1æ1æ¥ï¼å¤©æ´¥å¸å¬å®æºå³å¯¹æå¥å¬å¸æ¶å«ç»ç»ãé¢å¯¼ä¼ éæ´»å¨ç½ªåèåå¹¿åç½ªç«æ¡ä¾¦æ¥ãæ...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (7776, 400)\n",
      "x_test shape: (1945, 400)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 7776 samples, validate on 1945 samples\n",
      "Epoch 1/10\n",
      "7776/7776 [==============================] - 13s 2ms/step - loss: 0.4993 - acc: 0.7388 - val_loss: 0.4582 - val_acc: 0.7784\n",
      "Epoch 2/10\n",
      "7776/7776 [==============================] - 13s 2ms/step - loss: 0.3788 - acc: 0.8207 - val_loss: 0.4384 - val_acc: 0.7825\n",
      "Epoch 3/10\n",
      "7776/7776 [==============================] - 13s 2ms/step - loss: 0.3368 - acc: 0.8476 - val_loss: 0.4272 - val_acc: 0.7964\n",
      "Epoch 4/10\n",
      "7776/7776 [==============================] - 14s 2ms/step - loss: 0.2960 - acc: 0.8708 - val_loss: 0.4402 - val_acc: 0.7943\n",
      "Epoch 5/10\n",
      "7776/7776 [==============================] - 13s 2ms/step - loss: 0.2543 - acc: 0.8933 - val_loss: 0.4631 - val_acc: 0.7959\n",
      "Epoch 6/10\n",
      "7776/7776 [==============================] - 14s 2ms/step - loss: 0.2066 - acc: 0.9192 - val_loss: 0.4922 - val_acc: 0.7933\n",
      "accuracy is  0.7933161953727507\n",
      "precise is  0.8228279386712095\n",
      "recall is  0.8327586206896552\n",
      "auc is  0.8783999560729189\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "model = TextCNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "auc = roc_auc_score(y_test, result)\n",
    "\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "\n",
    "acc = accuracy_score(y_test, result)\n",
    "precise =  precision_score(y_test, result)\n",
    "recall = recall_score(y_test, result)\n",
    "\n",
    "print('accuracy is ', acc)\n",
    "print('precise is ', precise)\n",
    "print('recall is ', recall)\n",
    "print('auc is ', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 7776 samples, validate on 1945 samples\n",
      "Epoch 1/10\n",
      "7776/7776 [==============================] - 103s 13ms/step - loss: 0.4670 - acc: 0.7654 - val_loss: 0.4370 - val_acc: 0.7871\n",
      "Epoch 2/10\n",
      "7776/7776 [==============================] - 108s 14ms/step - loss: 0.3864 - acc: 0.8207 - val_loss: 0.4229 - val_acc: 0.7943\n",
      "Epoch 3/10\n",
      "7776/7776 [==============================] - 108s 14ms/step - loss: 0.3551 - acc: 0.8427 - val_loss: 0.4346 - val_acc: 0.7923\n",
      "Epoch 4/10\n",
      "7776/7776 [==============================] - 111s 14ms/step - loss: 0.3213 - acc: 0.8593 - val_loss: 0.4531 - val_acc: 0.7820\n",
      "Epoch 5/10\n",
      "7776/7776 [==============================] - 109s 14ms/step - loss: 0.2820 - acc: 0.8813 - val_loss: 0.5235 - val_acc: 0.7805\n",
      "Test...\n",
      "accuracy is  0.7804627249357327\n",
      "precise is  0.7749437359339835\n",
      "recall is  0.8905172413793103\n",
      "auc is  0.8758873270371184\n"
     ]
    }
   ],
   "source": [
    "model = RCNNVariant(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "auc = roc_auc_score(y_test, result)\n",
    "\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "\n",
    "acc = accuracy_score(y_test, result)\n",
    "precise =  precision_score(y_test, result)\n",
    "recall = recall_score(y_test, result)\n",
    "\n",
    "print('accuracy is ', acc)\n",
    "print('precise is ', precise)\n",
    "print('recall is ', recall)\n",
    "print('auc is ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCNN(ATTENTION+LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare input for model...\n",
      "x_train_current shape: (7776, 400)\n",
      "x_train_left shape: (7776, 400)\n",
      "x_train_right shape: (7776, 400)\n",
      "x_test_current shape: (1945, 400)\n",
      "x_test_left shape: (1945, 400)\n",
      "x_test_right shape: (1945, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 7776 samples, validate on 1945 samples\n",
      "Epoch 1/3\n",
      "7776/7776 [==============================] - 57s 7ms/step - loss: 0.4958 - acc: 0.7559 - val_loss: 0.4407 - val_acc: 0.7897\n",
      "Epoch 2/3\n",
      "7776/7776 [==============================] - 59s 8ms/step - loss: 0.3891 - acc: 0.8239 - val_loss: 0.4247 - val_acc: 0.7902\n",
      "Epoch 3/3\n",
      "7776/7776 [==============================] - 60s 8ms/step - loss: 0.3603 - acc: 0.8421 - val_loss: 0.4249 - val_acc: 0.7943\n",
      "Test...\n",
      "accuracy is  0.794344473007712\n",
      "precise is  0.8166666666666667\n",
      "recall is  0.8448275862068966\n",
      "auc is  0.8822457720184493\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 3\n",
    "\n",
    "print('Prepare input for model...')\n",
    "x_train_current = x_train\n",
    "x_train_left = np.hstack([np.expand_dims(x_train[:, 0], axis=1), x_train[:, 0:-1]])\n",
    "x_train_right = np.hstack([x_train[:, 1:], np.expand_dims(x_train[:, -1], axis=1)])\n",
    "x_test_current = x_test\n",
    "x_test_left = np.hstack([np.expand_dims(x_test[:, 0], axis=1), x_test[:, 0:-1]])\n",
    "x_test_right = np.hstack([x_test[:, 1:], np.expand_dims(x_test[:, -1], axis=1)])\n",
    "print('x_train_current shape:', x_train_current.shape)\n",
    "print('x_train_left shape:', x_train_left.shape)\n",
    "print('x_train_right shape:', x_train_right.shape)\n",
    "print('x_test_current shape:', x_test_current.shape)\n",
    "print('x_test_left shape:', x_test_left.shape)\n",
    "print('x_test_right shape:', x_test_right.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = RCNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit([x_train_current, x_train_left, x_train_right], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=([x_test_current, x_test_left, x_test_right], y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict([x_test_current, x_test_left, x_test_right])\n",
    "auc = roc_auc_score(y_test, result)\n",
    "\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "\n",
    "acc = accuracy_score(y_test, result)\n",
    "precise =  precision_score(y_test, result)\n",
    "recall = recall_score(y_test, result)\n",
    "\n",
    "print('accuracy is ', acc)\n",
    "print('precise is ', precise)\n",
    "print('recall is ', recall)\n",
    "print('auc is ', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (7776, 16, 25)\n",
      "x_test shape: (1945, 16, 25)\n",
      "Train...\n",
      "Train on 7776 samples, validate on 1945 samples\n",
      "Epoch 1/3\n",
      "7776/7776 [==============================] - 47s 6ms/step - loss: 0.4938 - acc: 0.7423 - val_loss: 0.4324 - val_acc: 0.7933\n",
      "Epoch 2/3\n",
      "7776/7776 [==============================] - 44s 6ms/step - loss: 0.3923 - acc: 0.8192 - val_loss: 0.4241 - val_acc: 0.8036\n",
      "Epoch 3/3\n",
      "7776/7776 [==============================] - 45s 6ms/step - loss: 0.3712 - acc: 0.8301 - val_loss: 0.4495 - val_acc: 0.7887\n",
      "Test...\n",
      "accuracy is  0.7886889460154242\n",
      "precise is  0.7826415094339623\n",
      "recall is  0.8939655172413793\n",
      "auc is  0.8845607291895454\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen_sentence = 16\n",
    "maxlen_word = 25\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 3\n",
    "\n",
    "x_train = x_train.reshape((len(x_train), maxlen_sentence, maxlen_word))\n",
    "x_test = x_test.reshape((len(x_test), maxlen_sentence, maxlen_word))\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "model = HAN(maxlen_sentence, maxlen_word, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "auc = roc_auc_score(y_test, result)\n",
    "\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "\n",
    "acc = accuracy_score(y_test, result)\n",
    "precise =  precision_score(y_test, result)\n",
    "recall = recall_score(y_test, result)\n",
    "\n",
    "print('accuracy is ', acc)\n",
    "print('precise is ', precise)\n",
    "print('recall is ', recall)\n",
    "print('auc is ', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a.id</th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "      <th>imgnum</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>title_sent2Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ffbe23538f7c9c15b1fb3bce338d39f1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://renderh5ui.inveno.com/detail.html?app=...</td>\n",
       "      <td>0</td>\n",
       "      <td>âæäºçå¥³å­©é½ä¸è¦å½©ç¤¼ââå©å©ï¼é£ä¸æ¯æäºï¼é£æ¯æ²¡èå­â</td>\n",
       "      <td>ç¨çè¯ææçè¯è¯­ï¼è®²è¿°å³äºå»ççå°ç¥è¯ï¼å¤§å®¶å¥½ï¼è¿éæ¯å»çå°åå£«ï¼æè°¢å¤§å®¶çè§çï¼å¸æå¤§...</td>\n",
       "      <td>[ 0.06967958  0.17589551 -0.01697676  0.072335...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86990225caf5829bc4b5f7b145a7e2ac</td>\n",
       "      <td>0</td>\n",
       "      <td>https://youliao.163yun.com/api-server/rss/xiao...</td>\n",
       "      <td>0</td>\n",
       "      <td>é»æ¯æ¸æå´æ°¸åå è¢«è°æ¥çç½ªèªæï¼å´æ°¸åææ°ååºè¯æèªå·±å®å¨</td>\n",
       "      <td>3æ1å·åæ¨ï¼é»æ¯æ¸çªç¶å¨ç¤¾äº¤å¹³å°ä¸­åæç§°âå´æ°¸åèªæäºâï¼å¼èµ·èè®ºä¸çåç¶ï¼ä¸ç¥éæ¯å¦ç...</td>\n",
       "      <td>[-0.04066085  0.22784638 -0.09804465 -0.159352...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fed8e7c9424115e07ce4860848440582</td>\n",
       "      <td>0</td>\n",
       "      <td>https://youliao.163yun.com/api-server/rss/xiao...</td>\n",
       "      <td>0</td>\n",
       "      <td>ç·å­å11ç½è´­äºä¸ç®±æ¹ä¾¿é¢ï¼æ¶å°å¿«éåï¼æå¼åè£¹ç´æ¥åäº</td>\n",
       "      <td>åä½ç½åå¤§å®¶å¥½ï¼æè°çæ´»ç¬æ¯ä¸åï¼ä»å¤©çæä»¬æ°¸è¿æ æ³é¢æµæå¤©ä¼åçä»ä¹ï¼ä¸çä¹å¤§æ¯å¤©é½ä¼æä¸...</td>\n",
       "      <td>[ 1.78375721e-01  1.86844319e-01 -1.13675095e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f8911a9e254d4fc2813d578ad8cd1cef</td>\n",
       "      <td>1</td>\n",
       "      <td>https://renderh5ui.inveno.com/detail.html?app=...</td>\n",
       "      <td>0</td>\n",
       "      <td>èå©æ¥æäºå²æåç«ï¼æå¯¹å¥¹è¯´äºä¸å¥è¯ï¼ç¬¬äºå¤©å°å§¨å­ä¸é¨æ¥éªæ</td>\n",
       "      <td>ç°å¨å¥³äººçæ¯å¾éº»ç¦ï¼å¶å®æä»¬ç·äººå¾æ¸æ¥ï¼å¥¹ä»¬å¥³äººæ¯ä¸ä¸ªææ¥è¿ç§äºæå¾éº»ç¦ï¼ä½æ¯...</td>\n",
       "      <td>[-0.00909939  0.10430206 -0.12136529 -0.030800...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9c4bc77498ec876614cb47bfd8edb978</td>\n",
       "      <td>0</td>\n",
       "      <td>https://xw.qq.com/partner/mibrowser/20190118A0...</td>\n",
       "      <td>3</td>\n",
       "      <td>æå¥é£æ³¢æªå¹³ï¼æ éæåè¢«æ¨ä¸é£å£æµªå°</td>\n",
       "      <td>2019å¹´1æ1æ¥ï¼å¤©æ´¥å¸å¬å®æºå³å¯¹æå¥å¬å¸æ¶å«ç»ç»ãé¢å¯¼ä¼ éæ´»å¨ç½ªåèåå¹¿åç½ªç«æ¡ä¾¦æ¥ãæ...</td>\n",
       "      <td>[-0.05588765  0.3731641  -0.21718842 -0.304607...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               a.id  label  \\\n",
       "0  ffbe23538f7c9c15b1fb3bce338d39f1      1   \n",
       "1  86990225caf5829bc4b5f7b145a7e2ac      0   \n",
       "2  fed8e7c9424115e07ce4860848440582      0   \n",
       "3  f8911a9e254d4fc2813d578ad8cd1cef      1   \n",
       "4  9c4bc77498ec876614cb47bfd8edb978      0   \n",
       "\n",
       "                                                 url  imgnum  \\\n",
       "0  https://renderh5ui.inveno.com/detail.html?app=...       0   \n",
       "1  https://youliao.163yun.com/api-server/rss/xiao...       0   \n",
       "2  https://youliao.163yun.com/api-server/rss/xiao...       0   \n",
       "3  https://renderh5ui.inveno.com/detail.html?app=...       0   \n",
       "4  https://xw.qq.com/partner/mibrowser/20190118A0...       3   \n",
       "\n",
       "                            title  \\\n",
       "0    âæäºçå¥³å­©é½ä¸è¦å½©ç¤¼ââå©å©ï¼é£ä¸æ¯æäºï¼é£æ¯æ²¡èå­â   \n",
       "1   é»æ¯æ¸æå´æ°¸åå è¢«è°æ¥çç½ªèªæï¼å´æ°¸åææ°ååºè¯æèªå·±å®å¨   \n",
       "2    ç·å­å11ç½è´­äºä¸ç®±æ¹ä¾¿é¢ï¼æ¶å°å¿«éåï¼æå¼åè£¹ç´æ¥åäº   \n",
       "3  èå©æ¥æäºå²æåç«ï¼æå¯¹å¥¹è¯´äºä¸å¥è¯ï¼ç¬¬äºå¤©å°å§¨å­ä¸é¨æ¥éªæ   \n",
       "4              æå¥é£æ³¢æªå¹³ï¼æ éæåè¢«æ¨ä¸é£å£æµªå°   \n",
       "\n",
       "                                                body  \\\n",
       "0    ç¨çè¯ææçè¯è¯­ï¼è®²è¿°å³äºå»ççå°ç¥è¯ï¼å¤§å®¶å¥½ï¼è¿éæ¯å»çå°åå£«ï¼æè°¢å¤§å®¶çè§çï¼å¸æå¤§...   \n",
       "1   3æ1å·åæ¨ï¼é»æ¯æ¸çªç¶å¨ç¤¾äº¤å¹³å°ä¸­åæç§°âå´æ°¸åèªæäºâï¼å¼èµ·èè®ºä¸çåç¶ï¼ä¸ç¥éæ¯å¦ç...   \n",
       "2  åä½ç½åå¤§å®¶å¥½ï¼æè°çæ´»ç¬æ¯ä¸åï¼ä»å¤©çæä»¬æ°¸è¿æ æ³é¢æµæå¤©ä¼åçä»ä¹ï¼ä¸çä¹å¤§æ¯å¤©é½ä¼æä¸...   \n",
       "3         ç°å¨å¥³äººçæ¯å¾éº»ç¦ï¼å¶å®æä»¬ç·äººå¾æ¸æ¥ï¼å¥¹ä»¬å¥³äººæ¯ä¸ä¸ªææ¥è¿ç§äºæå¾éº»ç¦ï¼ä½æ¯...   \n",
       "4  2019å¹´1æ1æ¥ï¼å¤©æ´¥å¸å¬å®æºå³å¯¹æå¥å¬å¸æ¶å«ç»ç»ãé¢å¯¼ä¼ éæ´»å¨ç½ªåèåå¹¿åç½ªç«æ¡ä¾¦æ¥ãæ...   \n",
       "\n",
       "                                      title_sent2Vec  \n",
       "0  [ 0.06967958  0.17589551 -0.01697676  0.072335...  \n",
       "1  [-0.04066085  0.22784638 -0.09804465 -0.159352...  \n",
       "2  [ 1.78375721e-01  1.86844319e-01 -1.13675095e-...  \n",
       "3  [-0.00909939  0.10430206 -0.12136529 -0.030800...  \n",
       "4  [-0.05588765  0.3731641  -0.21718842 -0.304607...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_embed = pd.read_csv('./data/data_sent.csv', sep='\\t')\n",
    "data_embed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing1(data):\n",
    "    # å¤çdata\n",
    "    cutWords_data = []\n",
    "    for article in data['title_sent2Vec'].astype(str):\n",
    "        vector_str = article.replace('[', '')\n",
    "        vector_str = vector_str.replace(']', '').strip()\n",
    "        vector_str = vector_str.split(\" \")\n",
    "        vector = []\n",
    "        for i in range(0, len(vector_str)):\n",
    "            if vector_str[i] != '':\n",
    "                if vector_str[i].endswith('\\n'):\n",
    "                    vector.append(vector_str[i].replace('\\n', ''))\n",
    "                else:\n",
    "                    vector.append(vector_str[i])\n",
    "        \n",
    "        \n",
    "        cutWords_data.append(vector)\n",
    "  \n",
    "    x_data = np.array(cutWords_data)\n",
    "    y_data = np.array(data['label'].values)\n",
    "\n",
    "    x_train, x_test, y_train,  y_test = train_test_split(x_data, y_data,test_size=0.2, random_state=42)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
