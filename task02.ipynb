{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class TextCNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        # Embedding part can try multichannel as same as origin paper\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        convs = []\n",
    "        for kernel_size in [3, 4, 5]:\n",
    "            c = Conv1D(128, kernel_size, activation='relu')(embedding)\n",
    "            c = GlobalMaxPooling1D()(c)\n",
    "            convs.append(c)\n",
    "        x = Concatenate()(convs)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "WARNING:tensorflow:From /home/angx/anaconda3/envs/spark_py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train...\n",
      "WARNING:tensorflow:From /home/angx/anaconda3/envs/spark_py35/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/angx/anaconda3/envs/spark_py35/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.3916 - acc: 0.8142 - val_loss: 0.3195 - val_acc: 0.8627\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.2114 - acc: 0.9160 - val_loss: 0.2531 - val_acc: 0.8962\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.1331 - acc: 0.9524 - val_loss: 0.2729 - val_acc: 0.8920\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 48s 2ms/step - loss: 0.0758 - acc: 0.9768 - val_loss: 0.3136 - val_acc: 0.8871\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 0.0351 - acc: 0.9924 - val_loss: 0.3732 - val_acc: 0.8872\n",
      "Test data accuracy is  0.88724\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = TextCNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "class TextRNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = LSTM(128)(embedding)  # LSTM or GRU\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 147s 6ms/step - loss: 0.5209 - acc: 0.7399 - val_loss: 0.6029 - val_acc: 0.6649\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 159s 6ms/step - loss: 0.3440 - acc: 0.8560 - val_loss: 0.3223 - val_acc: 0.8609\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 156s 6ms/step - loss: 0.2670 - acc: 0.8942 - val_loss: 0.3262 - val_acc: 0.8650\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 152s 6ms/step - loss: 0.2238 - acc: 0.9134 - val_loss: 0.3200 - val_acc: 0.8655\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 154s 6ms/step - loss: 0.1927 - acc: 0.9266 - val_loss: 0.3469 - val_acc: 0.8701\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 153s 6ms/step - loss: 0.1725 - acc: 0.9334 - val_loss: 0.3965 - val_acc: 0.8710\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.1373 - acc: 0.9493 - val_loss: 0.3988 - val_acc: 0.8573\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 153s 6ms/step - loss: 0.1366 - acc: 0.9482 - val_loss: 0.4446 - val_acc: 0.8590\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 152s 6ms/step - loss: 0.1133 - acc: 0.9575 - val_loss: 0.4195 - val_acc: 0.8686\n",
      "Test...\n",
      "Test data accuracy is  0.86864\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = TextRNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiTextRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class BiTextRNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = Bidirectional(LSTM(128))(embedding)  # LSTM or GRU\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 184s 7ms/step - loss: 0.6075 - acc: 0.6719 - val_loss: 0.6527 - val_acc: 0.6428\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 200s 8ms/step - loss: 0.4184 - acc: 0.8110 - val_loss: 0.3754 - val_acc: 0.8446\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 208s 8ms/step - loss: 0.3027 - acc: 0.8764 - val_loss: 0.3092 - val_acc: 0.8692\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 207s 8ms/step - loss: 0.2554 - acc: 0.8986 - val_loss: 0.3063 - val_acc: 0.8773\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 203s 8ms/step - loss: 0.2291 - acc: 0.9104 - val_loss: 0.2958 - val_acc: 0.8784\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 202s 8ms/step - loss: 0.1976 - acc: 0.9249 - val_loss: 0.3111 - val_acc: 0.8690\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 199s 8ms/step - loss: 0.1767 - acc: 0.9323 - val_loss: 0.3274 - val_acc: 0.8691\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 202s 8ms/step - loss: 0.1536 - acc: 0.9432 - val_loss: 0.3592 - val_acc: 0.8740\n",
      "Test...\n",
      "Test data accuracy is  0.87396\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = BiTextRNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM, Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class TextAttentionBiRNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True))(embedding)  # LSTM or GRU\n",
    "        x = Attention(self.maxlen)(x)\n",
    "        \n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 206s 8ms/step - loss: 0.4416 - acc: 0.7856 - val_loss: 0.3534 - val_acc: 0.8529\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 219s 9ms/step - loss: 0.2696 - acc: 0.8908 - val_loss: 0.2771 - val_acc: 0.8864\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 228s 9ms/step - loss: 0.2068 - acc: 0.9189 - val_loss: 0.2788 - val_acc: 0.8844\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 228s 9ms/step - loss: 0.1724 - acc: 0.9356 - val_loss: 0.2785 - val_acc: 0.8834\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 219s 9ms/step - loss: 0.1408 - acc: 0.9486 - val_loss: 0.2928 - val_acc: 0.8850\n",
      "Test...\n",
      "Test data accuracy is  0.88496\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = TextAttentionBiRNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding, Dense, SimpleRNN, Lambda, Concatenate, Conv1D, GlobalMaxPooling1D, LSTM\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "class RCNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input_current = Input((self.maxlen,))\n",
    "        input_left = Input((self.maxlen,))\n",
    "        input_right = Input((self.maxlen,))\n",
    "\n",
    "        embedder = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
    "        embedding_current = embedder(input_current)\n",
    "        embedding_left = embedder(input_left)\n",
    "        embedding_right = embedder(input_right)\n",
    "\n",
    "        x_left = LSTM(128, return_sequences=True)(embedding_left)\n",
    "        x_right = LSTM(128, return_sequences=True, go_backwards=True)(embedding_right)\n",
    "        x_right = Lambda(lambda x: K.reverse(x, axes=1))(x_right)\n",
    "        x = Concatenate(axis=2)([x_left, embedding_current, x_right])\n",
    "\n",
    "        x = Conv1D(64, kernel_size=1, activation='tanh')(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=[input_current, input_left, input_right], outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Prepare input for model...\n",
      "x_train_current shape: (25000, 400)\n",
      "x_train_left shape: (25000, 400)\n",
      "x_train_right shape: (25000, 400)\n",
      "x_test_current shape: (25000, 400)\n",
      "x_test_left shape: (25000, 400)\n",
      "x_test_right shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 224s 9ms/step - loss: 0.4024 - acc: 0.8178 - val_loss: 0.2848 - val_acc: 0.8806\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 229s 9ms/step - loss: 0.2363 - acc: 0.9056 - val_loss: 0.2576 - val_acc: 0.8965\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 228s 9ms/step - loss: 0.1699 - acc: 0.9348 - val_loss: 0.2542 - val_acc: 0.8980\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 232s 9ms/step - loss: 0.1238 - acc: 0.9558 - val_loss: 0.2962 - val_acc: 0.8894\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 235s 9ms/step - loss: 0.0774 - acc: 0.9744 - val_loss: 0.3308 - val_acc: 0.8894\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 227s 9ms/step - loss: 0.0459 - acc: 0.9851 - val_loss: 0.4009 - val_acc: 0.8849\n",
      "Test...\n",
      "Test data accuracy is  0.88492\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Prepare input for model...')\n",
    "x_train_current = x_train\n",
    "x_train_left = np.hstack([np.expand_dims(x_train[:, 0], axis=1), x_train[:, 0:-1]])\n",
    "x_train_right = np.hstack([x_train[:, 1:], np.expand_dims(x_train[:, -1], axis=1)])\n",
    "x_test_current = x_test\n",
    "x_test_left = np.hstack([np.expand_dims(x_test[:, 0], axis=1), x_test[:, 0:-1]])\n",
    "x_test_right = np.hstack([x_test[:, 1:], np.expand_dims(x_test[:, -1], axis=1)])\n",
    "print('x_train_current shape:', x_train_current.shape)\n",
    "print('x_train_left shape:', x_train_left.shape)\n",
    "print('x_train_right shape:', x_train_right.shape)\n",
    "print('x_test_current shape:', x_test_current.shape)\n",
    "print('x_test_left shape:', x_test_left.shape)\n",
    "print('x_test_right shape:', x_test_right.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = RCNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit([x_train_current, x_train_left, x_train_right], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=([x_test_current, x_test_left, x_test_right], y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict([x_test_current, x_test_left, x_test_right])\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCNN (Attention + BiLstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding, Dense, SimpleRNN, Lambda, Concatenate, Conv1D, GlobalMaxPooling1D, LSTM\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "\n",
    "\n",
    "class RCNN_Att_BiLstm(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input_current = Input((self.maxlen,))\n",
    "        input_left = Input((self.maxlen,))\n",
    "        input_right = Input((self.maxlen,))\n",
    "\n",
    "        embedder = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
    "        embedding_current = embedder(input_current)\n",
    "        embedding_left = embedder(input_left)\n",
    "        embedding_right = embedder(input_right)\n",
    "        \n",
    "        x_left = Bidirectional(LSTM(128, return_sequences=True))(embedding_left)  # LSTM or GRU\n",
    "        x_left = Attention(self.maxlen)(x_left)\n",
    "        \n",
    "        x_right = Bidirectional(LSTM(128, return_sequences=True))(embedding_right)  # LSTM or GRU\n",
    "        x_right = Attention(self.maxlen)(x_right)\n",
    "\n",
    "        #x_left = LSTM(128, return_sequences=True)(embedding_left)\n",
    "        #x_right = LSTM(128, return_sequences=True, go_backwards=True)(embedding_right)\n",
    "        x_right = Lambda(lambda x: K.reverse(x, axes=1))(x_right)\n",
    "        \n",
    "        x = Concatenate(axis=2)([x_left, embedding_current, x_right])\n",
    "\n",
    "        x = Conv1D(64, kernel_size=1, activation='tanh')(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=[input_current, input_left, input_right], outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Prepare input for model...\n",
      "x_train_current shape: (25000, 400)\n",
      "x_train_left shape: (25000, 400)\n",
      "x_train_right shape: (25000, 400)\n",
      "x_test_current shape: (25000, 400)\n",
      "x_test_left shape: (25000, 400)\n",
      "x_test_right shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 222s 9ms/step - loss: 0.3931 - acc: 0.8187 - val_loss: 0.2986 - val_acc: 0.8729\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 230s 9ms/step - loss: 0.2336 - acc: 0.9054 - val_loss: 0.2669 - val_acc: 0.8896\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 232s 9ms/step - loss: 0.1691 - acc: 0.9364 - val_loss: 0.2684 - val_acc: 0.8916\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 237s 9ms/step - loss: 0.1163 - acc: 0.9596 - val_loss: 0.3161 - val_acc: 0.8829\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 226s 9ms/step - loss: 0.0761 - acc: 0.9730 - val_loss: 0.3681 - val_acc: 0.8804\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 227s 9ms/step - loss: 0.0472 - acc: 0.9839 - val_loss: 0.4083 - val_acc: 0.8818\n",
      "Test...\n",
      "Test data accuracy is  0.8818\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Prepare input for model...')\n",
    "x_train_current = x_train\n",
    "x_train_left = np.hstack([np.expand_dims(x_train[:, 0], axis=1), x_train[:, 0:-1]])\n",
    "x_train_right = np.hstack([x_train[:, 1:], np.expand_dims(x_train[:, -1], axis=1)])\n",
    "x_test_current = x_test\n",
    "x_test_left = np.hstack([np.expand_dims(x_test[:, 0], axis=1), x_test[:, 0:-1]])\n",
    "x_test_right = np.hstack([x_test[:, 1:], np.expand_dims(x_test[:, -1], axis=1)])\n",
    "print('x_train_current shape:', x_train_current.shape)\n",
    "print('x_train_left shape:', x_train_left.shape)\n",
    "print('x_train_right shape:', x_train_right.shape)\n",
    "print('x_test_current shape:', x_test_current.shape)\n",
    "print('x_test_left shape:', x_test_left.shape)\n",
    "print('x_test_right shape:', x_test_right.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = RCNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit([x_train_current, x_train_left, x_train_right], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=([x_test_current, x_test_left, x_test_right], y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict([x_test_current, x_test_left, x_test_right])\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCNNVariant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Concatenate, Conv1D, Bidirectional, LSTM, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "class RCNNVariant(object):\n",
    "    \"\"\"Variant of RCNN.\n",
    "        Base on structure of RCNN, we do some improvement:\n",
    "        1. Ignore the shift for left/right context.\n",
    "        2. Use Bidirectional LSTM/GRU to encode context.\n",
    "        3. Use Multi-CNN to represent the semantic vectors.\n",
    "        4. Use ReLU instead of Tanh.\n",
    "        5. Use both AveragePooling and MaxPooling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "\n",
    "        x_context = Bidirectional(LSTM(128, return_sequences=True))(embedding)\n",
    "        x = Concatenate()([embedding, x_context])\n",
    "\n",
    "        convs = []\n",
    "        for kernel_size in range(1, 5):\n",
    "            conv = Conv1D(128, kernel_size, activation='relu')(x)\n",
    "            convs.append(conv)\n",
    "        poolings = [GlobalAveragePooling1D()(conv) for conv in convs] + [GlobalMaxPooling1D()(conv) for conv in convs]\n",
    "        x = Concatenate()(poolings)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 389s 16ms/step - loss: 0.3569 - acc: 0.8370 - val_loss: 0.2580 - val_acc: 0.8932\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 397s 16ms/step - loss: 0.2030 - acc: 0.9220 - val_loss: 0.2470 - val_acc: 0.8983\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 385s 15ms/step - loss: 0.1520 - acc: 0.9412 - val_loss: 0.2626 - val_acc: 0.8954\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 389s 16ms/step - loss: 0.1014 - acc: 0.9647 - val_loss: 0.3091 - val_acc: 0.8868\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 386s 15ms/step - loss: 0.0616 - acc: 0.9792 - val_loss: 0.4233 - val_acc: 0.8780\n",
      "Test...\n",
      "Test data accuracy is  0.87796\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = RCNNVariant(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Bidirectional, LSTM, TimeDistributed\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence = Attention()(hidden)\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        e = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))  # e = K.dot(x, self.W)\n",
    "        if self.bias:\n",
    "            e += self.b\n",
    "        e = K.tanh(e)\n",
    "\n",
    "        a = K.exp(e)\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "\n",
    "        c = K.sum(a * x, axis=1)\n",
    "        return c\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "    \n",
    "class HAN(object):\n",
    "    def __init__(self, maxlen_sentence, maxlen_word, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen_sentence = maxlen_sentence\n",
    "        self.maxlen_word = maxlen_word\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        # Word part\n",
    "        input_word = Input(shape=(self.maxlen_word,))\n",
    "        x_word = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen_word)(input_word)\n",
    "        x_word = Bidirectional(LSTM(128, return_sequences=True))(x_word)  # LSTM or GRU\n",
    "        x_word = Attention(self.maxlen_word)(x_word)\n",
    "        model_word = Model(input_word, x_word)\n",
    "\n",
    "        # Sentence part\n",
    "        input = Input(shape=(self.maxlen_sentence, self.maxlen_word))\n",
    "        x_sentence = TimeDistributed(model_word)(input)\n",
    "        x_sentence = Bidirectional(LSTM(128, return_sequences=True))(x_sentence)  # LSTM or GRU\n",
    "        x_sentence = Attention(self.maxlen_sentence)(x_sentence)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x_sentence)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x #sentence x #word)...\n",
      "x_train shape: (25000, 16, 25)\n",
      "x_test shape: (25000, 16, 25)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 160s 6ms/step - loss: 0.3617 - acc: 0.8330 - val_loss: 0.3229 - val_acc: 0.8607\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 164s 7ms/step - loss: 0.2338 - acc: 0.9078 - val_loss: 0.2705 - val_acc: 0.8865\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 172s 7ms/step - loss: 0.1882 - acc: 0.9293 - val_loss: 0.2873 - val_acc: 0.8844\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 171s 7ms/step - loss: 0.1501 - acc: 0.9457 - val_loss: 0.2986 - val_acc: 0.8859\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 171s 7ms/step - loss: 0.1141 - acc: 0.9608 - val_loss: 0.3329 - val_acc: 0.8839\n",
      "Test...\n",
      "Test data accuracy is  0.88392\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen_sentence = 16\n",
    "maxlen_word = 25\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x #sentence x #word)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen_sentence * maxlen_word)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen_sentence * maxlen_word)\n",
    "x_train = x_train.reshape((len(x_train), maxlen_sentence, maxlen_word))\n",
    "x_test = x_test.reshape((len(x_test), maxlen_sentence, maxlen_word))\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = HAN(maxlen_sentence, maxlen_word, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "class FastText(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = GlobalAveragePooling1D()(embedding)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Average train sequence length: 238\n",
      "Average test sequence length: 230\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/50\n",
      "25000/25000 [==============================] - 4s 141us/step - loss: 0.6170 - acc: 0.7280 - val_loss: 0.5125 - val_acc: 0.8145\n",
      "Epoch 2/50\n",
      "25000/25000 [==============================] - 3s 115us/step - loss: 0.4228 - acc: 0.8535 - val_loss: 0.3802 - val_acc: 0.8615\n",
      "Epoch 3/50\n",
      "25000/25000 [==============================] - 3s 116us/step - loss: 0.3311 - acc: 0.8796 - val_loss: 0.3284 - val_acc: 0.8745\n",
      "Epoch 4/50\n",
      "25000/25000 [==============================] - 3s 118us/step - loss: 0.2885 - acc: 0.8924 - val_loss: 0.3042 - val_acc: 0.8827\n",
      "Epoch 5/50\n",
      "25000/25000 [==============================] - 3s 115us/step - loss: 0.2621 - acc: 0.9028 - val_loss: 0.2909 - val_acc: 0.8844\n",
      "Epoch 6/50\n",
      "25000/25000 [==============================] - 3s 115us/step - loss: 0.2441 - acc: 0.9096 - val_loss: 0.2858 - val_acc: 0.8858\n",
      "Epoch 7/50\n",
      "25000/25000 [==============================] - 3s 117us/step - loss: 0.2292 - acc: 0.9144 - val_loss: 0.2814 - val_acc: 0.8869\n",
      "Epoch 8/50\n",
      "25000/25000 [==============================] - 3s 118us/step - loss: 0.2179 - acc: 0.9189 - val_loss: 0.2798 - val_acc: 0.8878\n",
      "Epoch 9/50\n",
      "25000/25000 [==============================] - 3s 121us/step - loss: 0.2085 - acc: 0.9227 - val_loss: 0.2839 - val_acc: 0.8854\n",
      "Epoch 10/50\n",
      "25000/25000 [==============================] - 3s 118us/step - loss: 0.2010 - acc: 0.9266 - val_loss: 0.2830 - val_acc: 0.8866\n",
      "Epoch 11/50\n",
      "25000/25000 [==============================] - 3s 116us/step - loss: 0.1941 - acc: 0.9292 - val_loss: 0.2860 - val_acc: 0.8860\n",
      "Test...\n",
      "Test data accuracy is  0.886\n"
     ]
    }
   ],
   "source": [
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    # >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    # >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "\n",
    "# Set parameters:\n",
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 1\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 50\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = FastText(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import jieba\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def preprocessing(data):\n",
    "    stopword_list = [k.strip() for k in open('./data/stop_words.utf8', encoding='utf8').readlines() if k.strip() != '']\n",
    "    # 处理data\n",
    "    cutWords_data = []\n",
    "    for article in data['body'].astype(str):\n",
    "        cutWords = [k for k in jieba.cut(article) if k not in stopword_list]\n",
    "        cutWords_data.append(cutWords)\n",
    "    tokenizer = Tokenizer(num_words=1000)  # 建立一个2000个单词的字典\n",
    "    tokenizer.fit_on_texts(cutWords_data)\n",
    "    # 对每一篇文章转换为数字列表，使用每个词的编号进行编号\n",
    "    x_data_seq = tokenizer.texts_to_sequences(cutWords_data)\n",
    "    x_data = sequence.pad_sequences(x_data_seq, maxlen=400)\n",
    "    y_data = np.array(data['label'].values)\n",
    "\n",
    "    x_train, x_test, y_train,  y_test = train_test_split(x_data, y_data,test_size=0.2, random_state=42)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 水文标注预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./data/water_artical.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a.id</th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "      <th>imgnum</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ffbe23538f7c9c15b1fb3bce338d39f1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://renderh5ui.inveno.com/detail.html?app=...</td>\n",
       "      <td>0</td>\n",
       "      <td>“懂事的女孩都不要彩礼”“婆婆，那不是懂事，那是没脑子”</td>\n",
       "      <td>用真诚易懂的话语，讲述关于养生的小知识，大家好，这里是养生小博士，感谢大家的观看，希望大...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86990225caf5829bc4b5f7b145a7e2ac</td>\n",
       "      <td>0</td>\n",
       "      <td>https://youliao.163yun.com/api-server/rss/xiao...</td>\n",
       "      <td>0</td>\n",
       "      <td>黄毅清曝崔永元因被调查畏罪自杀？崔永元最新回应证明自己安全</td>\n",
       "      <td>3月1号凌晨，黄毅清突然在社交平台中发文称“崔永元自杀了”，引起舆论一片哗然，不知道是否真...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fed8e7c9424115e07ce4860848440582</td>\n",
       "      <td>0</td>\n",
       "      <td>https://youliao.163yun.com/api-server/rss/xiao...</td>\n",
       "      <td>0</td>\n",
       "      <td>男子双11网购了一箱方便面，收到快递后，打开包裹直接吐了</td>\n",
       "      <td>各位网友大家好，所谓生活瞬息万变，今天的我们永远无法预测明天会发生什么，世界之大每天都会有不...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f8911a9e254d4fc2813d578ad8cd1cef</td>\n",
       "      <td>1</td>\n",
       "      <td>https://renderh5ui.inveno.com/detail.html?app=...</td>\n",
       "      <td>0</td>\n",
       "      <td>老婆来月事冲我发火，我对她说了一句话，第二天小姨子上门来骂我</td>\n",
       "      <td>现在女人真是很麻烦，其实我们男人很清楚，她们女人每一个月来这种事情很麻烦，但是...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9c4bc77498ec876614cb47bfd8edb978</td>\n",
       "      <td>0</td>\n",
       "      <td>https://xw.qq.com/partner/mibrowser/20190118A0...</td>\n",
       "      <td>3</td>\n",
       "      <td>权健风波未平，无限极又被推上风口浪尖</td>\n",
       "      <td>2019年1月1日，天津市公安机关对权健公司涉嫌组织、领导传销活动罪和虚假广告罪立案侦查。最...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               a.id  label  \\\n",
       "0  ffbe23538f7c9c15b1fb3bce338d39f1      1   \n",
       "1  86990225caf5829bc4b5f7b145a7e2ac      0   \n",
       "2  fed8e7c9424115e07ce4860848440582      0   \n",
       "3  f8911a9e254d4fc2813d578ad8cd1cef      1   \n",
       "4  9c4bc77498ec876614cb47bfd8edb978      0   \n",
       "\n",
       "                                                 url  imgnum  \\\n",
       "0  https://renderh5ui.inveno.com/detail.html?app=...       0   \n",
       "1  https://youliao.163yun.com/api-server/rss/xiao...       0   \n",
       "2  https://youliao.163yun.com/api-server/rss/xiao...       0   \n",
       "3  https://renderh5ui.inveno.com/detail.html?app=...       0   \n",
       "4  https://xw.qq.com/partner/mibrowser/20190118A0...       3   \n",
       "\n",
       "                            title  \\\n",
       "0    “懂事的女孩都不要彩礼”“婆婆，那不是懂事，那是没脑子”   \n",
       "1   黄毅清曝崔永元因被调查畏罪自杀？崔永元最新回应证明自己安全   \n",
       "2    男子双11网购了一箱方便面，收到快递后，打开包裹直接吐了   \n",
       "3  老婆来月事冲我发火，我对她说了一句话，第二天小姨子上门来骂我   \n",
       "4              权健风波未平，无限极又被推上风口浪尖   \n",
       "\n",
       "                                                body  \n",
       "0    用真诚易懂的话语，讲述关于养生的小知识，大家好，这里是养生小博士，感谢大家的观看，希望大...  \n",
       "1   3月1号凌晨，黄毅清突然在社交平台中发文称“崔永元自杀了”，引起舆论一片哗然，不知道是否真...  \n",
       "2  各位网友大家好，所谓生活瞬息万变，今天的我们永远无法预测明天会发生什么，世界之大每天都会有不...  \n",
       "3         现在女人真是很麻烦，其实我们男人很清楚，她们女人每一个月来这种事情很麻烦，但是...  \n",
       "4  2019年1月1日，天津市公安机关对权健公司涉嫌组织、领导传销活动罪和虚假广告罪立案侦查。最...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (7776, 400)\n",
      "x_test shape: (1945, 400)\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 7776 samples, validate on 1945 samples\n",
      "Epoch 1/10\n",
      "7776/7776 [==============================] - 13s 2ms/step - loss: 0.4993 - acc: 0.7388 - val_loss: 0.4582 - val_acc: 0.7784\n",
      "Epoch 2/10\n",
      "7776/7776 [==============================] - 13s 2ms/step - loss: 0.3788 - acc: 0.8207 - val_loss: 0.4384 - val_acc: 0.7825\n",
      "Epoch 3/10\n",
      "7776/7776 [==============================] - 13s 2ms/step - loss: 0.3368 - acc: 0.8476 - val_loss: 0.4272 - val_acc: 0.7964\n",
      "Epoch 4/10\n",
      "7776/7776 [==============================] - 14s 2ms/step - loss: 0.2960 - acc: 0.8708 - val_loss: 0.4402 - val_acc: 0.7943\n",
      "Epoch 5/10\n",
      "7776/7776 [==============================] - 13s 2ms/step - loss: 0.2543 - acc: 0.8933 - val_loss: 0.4631 - val_acc: 0.7959\n",
      "Epoch 6/10\n",
      "7776/7776 [==============================] - 14s 2ms/step - loss: 0.2066 - acc: 0.9192 - val_loss: 0.4922 - val_acc: 0.7933\n",
      "accuracy is  0.7933161953727507\n",
      "precise is  0.8228279386712095\n",
      "recall is  0.8327586206896552\n",
      "auc is  0.8783999560729189\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "model = TextCNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "auc = roc_auc_score(y_test, result)\n",
    "\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "\n",
    "acc = accuracy_score(y_test, result)\n",
    "precise =  precision_score(y_test, result)\n",
    "recall = recall_score(y_test, result)\n",
    "\n",
    "print('accuracy is ', acc)\n",
    "print('precise is ', precise)\n",
    "print('recall is ', recall)\n",
    "print('auc is ', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 7776 samples, validate on 1945 samples\n",
      "Epoch 1/10\n",
      "7776/7776 [==============================] - 103s 13ms/step - loss: 0.4670 - acc: 0.7654 - val_loss: 0.4370 - val_acc: 0.7871\n",
      "Epoch 2/10\n",
      "7776/7776 [==============================] - 108s 14ms/step - loss: 0.3864 - acc: 0.8207 - val_loss: 0.4229 - val_acc: 0.7943\n",
      "Epoch 3/10\n",
      "7776/7776 [==============================] - 108s 14ms/step - loss: 0.3551 - acc: 0.8427 - val_loss: 0.4346 - val_acc: 0.7923\n",
      "Epoch 4/10\n",
      "7776/7776 [==============================] - 111s 14ms/step - loss: 0.3213 - acc: 0.8593 - val_loss: 0.4531 - val_acc: 0.7820\n",
      "Epoch 5/10\n",
      "7776/7776 [==============================] - 109s 14ms/step - loss: 0.2820 - acc: 0.8813 - val_loss: 0.5235 - val_acc: 0.7805\n",
      "Test...\n",
      "accuracy is  0.7804627249357327\n",
      "precise is  0.7749437359339835\n",
      "recall is  0.8905172413793103\n",
      "auc is  0.8758873270371184\n"
     ]
    }
   ],
   "source": [
    "model = RCNNVariant(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "auc = roc_auc_score(y_test, result)\n",
    "\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "\n",
    "acc = accuracy_score(y_test, result)\n",
    "precise =  precision_score(y_test, result)\n",
    "recall = recall_score(y_test, result)\n",
    "\n",
    "print('accuracy is ', acc)\n",
    "print('precise is ', precise)\n",
    "print('recall is ', recall)\n",
    "print('auc is ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCNN(ATTENTION+LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare input for model...\n",
      "x_train_current shape: (7776, 400)\n",
      "x_train_left shape: (7776, 400)\n",
      "x_train_right shape: (7776, 400)\n",
      "x_test_current shape: (1945, 400)\n",
      "x_test_left shape: (1945, 400)\n",
      "x_test_right shape: (1945, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 7776 samples, validate on 1945 samples\n",
      "Epoch 1/3\n",
      "7776/7776 [==============================] - 57s 7ms/step - loss: 0.4958 - acc: 0.7559 - val_loss: 0.4407 - val_acc: 0.7897\n",
      "Epoch 2/3\n",
      "7776/7776 [==============================] - 59s 8ms/step - loss: 0.3891 - acc: 0.8239 - val_loss: 0.4247 - val_acc: 0.7902\n",
      "Epoch 3/3\n",
      "7776/7776 [==============================] - 60s 8ms/step - loss: 0.3603 - acc: 0.8421 - val_loss: 0.4249 - val_acc: 0.7943\n",
      "Test...\n",
      "accuracy is  0.794344473007712\n",
      "precise is  0.8166666666666667\n",
      "recall is  0.8448275862068966\n",
      "auc is  0.8822457720184493\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 3\n",
    "\n",
    "print('Prepare input for model...')\n",
    "x_train_current = x_train\n",
    "x_train_left = np.hstack([np.expand_dims(x_train[:, 0], axis=1), x_train[:, 0:-1]])\n",
    "x_train_right = np.hstack([x_train[:, 1:], np.expand_dims(x_train[:, -1], axis=1)])\n",
    "x_test_current = x_test\n",
    "x_test_left = np.hstack([np.expand_dims(x_test[:, 0], axis=1), x_test[:, 0:-1]])\n",
    "x_test_right = np.hstack([x_test[:, 1:], np.expand_dims(x_test[:, -1], axis=1)])\n",
    "print('x_train_current shape:', x_train_current.shape)\n",
    "print('x_train_left shape:', x_train_left.shape)\n",
    "print('x_train_right shape:', x_train_right.shape)\n",
    "print('x_test_current shape:', x_test_current.shape)\n",
    "print('x_test_left shape:', x_test_left.shape)\n",
    "print('x_test_right shape:', x_test_right.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = RCNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit([x_train_current, x_train_left, x_train_right], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=([x_test_current, x_test_left, x_test_right], y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict([x_test_current, x_test_left, x_test_right])\n",
    "auc = roc_auc_score(y_test, result)\n",
    "\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "\n",
    "acc = accuracy_score(y_test, result)\n",
    "precise =  precision_score(y_test, result)\n",
    "recall = recall_score(y_test, result)\n",
    "\n",
    "print('accuracy is ', acc)\n",
    "print('precise is ', precise)\n",
    "print('recall is ', recall)\n",
    "print('auc is ', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (7776, 16, 25)\n",
      "x_test shape: (1945, 16, 25)\n",
      "Train...\n",
      "Train on 7776 samples, validate on 1945 samples\n",
      "Epoch 1/3\n",
      "7776/7776 [==============================] - 47s 6ms/step - loss: 0.4938 - acc: 0.7423 - val_loss: 0.4324 - val_acc: 0.7933\n",
      "Epoch 2/3\n",
      "7776/7776 [==============================] - 44s 6ms/step - loss: 0.3923 - acc: 0.8192 - val_loss: 0.4241 - val_acc: 0.8036\n",
      "Epoch 3/3\n",
      "7776/7776 [==============================] - 45s 6ms/step - loss: 0.3712 - acc: 0.8301 - val_loss: 0.4495 - val_acc: 0.7887\n",
      "Test...\n",
      "accuracy is  0.7886889460154242\n",
      "precise is  0.7826415094339623\n",
      "recall is  0.8939655172413793\n",
      "auc is  0.8845607291895454\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen_sentence = 16\n",
    "maxlen_word = 25\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 3\n",
    "\n",
    "x_train = x_train.reshape((len(x_train), maxlen_sentence, maxlen_word))\n",
    "x_test = x_test.reshape((len(x_test), maxlen_sentence, maxlen_word))\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "model = HAN(maxlen_sentence, maxlen_word, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "auc = roc_auc_score(y_test, result)\n",
    "\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "\n",
    "acc = accuracy_score(y_test, result)\n",
    "precise =  precision_score(y_test, result)\n",
    "recall = recall_score(y_test, result)\n",
    "\n",
    "print('accuracy is ', acc)\n",
    "print('precise is ', precise)\n",
    "print('recall is ', recall)\n",
    "print('auc is ', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a.id</th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "      <th>imgnum</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>title_sent2Vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ffbe23538f7c9c15b1fb3bce338d39f1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://renderh5ui.inveno.com/detail.html?app=...</td>\n",
       "      <td>0</td>\n",
       "      <td>“懂事的女孩都不要彩礼”“婆婆，那不是懂事，那是没脑子”</td>\n",
       "      <td>用真诚易懂的话语，讲述关于养生的小知识，大家好，这里是养生小博士，感谢大家的观看，希望大...</td>\n",
       "      <td>[ 0.06967958  0.17589551 -0.01697676  0.072335...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86990225caf5829bc4b5f7b145a7e2ac</td>\n",
       "      <td>0</td>\n",
       "      <td>https://youliao.163yun.com/api-server/rss/xiao...</td>\n",
       "      <td>0</td>\n",
       "      <td>黄毅清曝崔永元因被调查畏罪自杀？崔永元最新回应证明自己安全</td>\n",
       "      <td>3月1号凌晨，黄毅清突然在社交平台中发文称“崔永元自杀了”，引起舆论一片哗然，不知道是否真...</td>\n",
       "      <td>[-0.04066085  0.22784638 -0.09804465 -0.159352...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fed8e7c9424115e07ce4860848440582</td>\n",
       "      <td>0</td>\n",
       "      <td>https://youliao.163yun.com/api-server/rss/xiao...</td>\n",
       "      <td>0</td>\n",
       "      <td>男子双11网购了一箱方便面，收到快递后，打开包裹直接吐了</td>\n",
       "      <td>各位网友大家好，所谓生活瞬息万变，今天的我们永远无法预测明天会发生什么，世界之大每天都会有不...</td>\n",
       "      <td>[ 1.78375721e-01  1.86844319e-01 -1.13675095e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f8911a9e254d4fc2813d578ad8cd1cef</td>\n",
       "      <td>1</td>\n",
       "      <td>https://renderh5ui.inveno.com/detail.html?app=...</td>\n",
       "      <td>0</td>\n",
       "      <td>老婆来月事冲我发火，我对她说了一句话，第二天小姨子上门来骂我</td>\n",
       "      <td>现在女人真是很麻烦，其实我们男人很清楚，她们女人每一个月来这种事情很麻烦，但是...</td>\n",
       "      <td>[-0.00909939  0.10430206 -0.12136529 -0.030800...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9c4bc77498ec876614cb47bfd8edb978</td>\n",
       "      <td>0</td>\n",
       "      <td>https://xw.qq.com/partner/mibrowser/20190118A0...</td>\n",
       "      <td>3</td>\n",
       "      <td>权健风波未平，无限极又被推上风口浪尖</td>\n",
       "      <td>2019年1月1日，天津市公安机关对权健公司涉嫌组织、领导传销活动罪和虚假广告罪立案侦查。最...</td>\n",
       "      <td>[-0.05588765  0.3731641  -0.21718842 -0.304607...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               a.id  label  \\\n",
       "0  ffbe23538f7c9c15b1fb3bce338d39f1      1   \n",
       "1  86990225caf5829bc4b5f7b145a7e2ac      0   \n",
       "2  fed8e7c9424115e07ce4860848440582      0   \n",
       "3  f8911a9e254d4fc2813d578ad8cd1cef      1   \n",
       "4  9c4bc77498ec876614cb47bfd8edb978      0   \n",
       "\n",
       "                                                 url  imgnum  \\\n",
       "0  https://renderh5ui.inveno.com/detail.html?app=...       0   \n",
       "1  https://youliao.163yun.com/api-server/rss/xiao...       0   \n",
       "2  https://youliao.163yun.com/api-server/rss/xiao...       0   \n",
       "3  https://renderh5ui.inveno.com/detail.html?app=...       0   \n",
       "4  https://xw.qq.com/partner/mibrowser/20190118A0...       3   \n",
       "\n",
       "                            title  \\\n",
       "0    “懂事的女孩都不要彩礼”“婆婆，那不是懂事，那是没脑子”   \n",
       "1   黄毅清曝崔永元因被调查畏罪自杀？崔永元最新回应证明自己安全   \n",
       "2    男子双11网购了一箱方便面，收到快递后，打开包裹直接吐了   \n",
       "3  老婆来月事冲我发火，我对她说了一句话，第二天小姨子上门来骂我   \n",
       "4              权健风波未平，无限极又被推上风口浪尖   \n",
       "\n",
       "                                                body  \\\n",
       "0    用真诚易懂的话语，讲述关于养生的小知识，大家好，这里是养生小博士，感谢大家的观看，希望大...   \n",
       "1   3月1号凌晨，黄毅清突然在社交平台中发文称“崔永元自杀了”，引起舆论一片哗然，不知道是否真...   \n",
       "2  各位网友大家好，所谓生活瞬息万变，今天的我们永远无法预测明天会发生什么，世界之大每天都会有不...   \n",
       "3         现在女人真是很麻烦，其实我们男人很清楚，她们女人每一个月来这种事情很麻烦，但是...   \n",
       "4  2019年1月1日，天津市公安机关对权健公司涉嫌组织、领导传销活动罪和虚假广告罪立案侦查。最...   \n",
       "\n",
       "                                      title_sent2Vec  \n",
       "0  [ 0.06967958  0.17589551 -0.01697676  0.072335...  \n",
       "1  [-0.04066085  0.22784638 -0.09804465 -0.159352...  \n",
       "2  [ 1.78375721e-01  1.86844319e-01 -1.13675095e-...  \n",
       "3  [-0.00909939  0.10430206 -0.12136529 -0.030800...  \n",
       "4  [-0.05588765  0.3731641  -0.21718842 -0.304607...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_embed = pd.read_csv('./data/data_sent.csv', sep='\\t')\n",
    "data_embed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing1(data):\n",
    "    # 处理data\n",
    "    cutWords_data = []\n",
    "    for article in data['title_sent2Vec'].astype(str):\n",
    "        vector_str = article.replace('[', '')\n",
    "        vector_str = vector_str.replace(']', '').strip()\n",
    "        vector_str = vector_str.split(\" \")\n",
    "        vector = []\n",
    "        for i in range(0, len(vector_str)):\n",
    "            if vector_str[i] != '':\n",
    "                if vector_str[i].endswith('\\n'):\n",
    "                    vector.append(vector_str[i].replace('\\n', ''))\n",
    "                else:\n",
    "                    vector.append(vector_str[i])\n",
    "        \n",
    "        \n",
    "        cutWords_data.append(vector)\n",
    "  \n",
    "    x_data = np.array(cutWords_data)\n",
    "    y_data = np.array(data['label'].values)\n",
    "\n",
    "    x_train, x_test, y_train,  y_test = train_test_split(x_data, y_data,test_size=0.2, random_state=42)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
