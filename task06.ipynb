{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "class FastText(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = GlobalAveragePooling1D()(embedding)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    \"\"\"\n",
    "    Extract a set of n-grams from a list of integers.\n",
    "    # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "    # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "    \"\"\"\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    \"\"\"\n",
    "    Augment the input list of list (sequences) by appending n-grams values.\n",
    "    Example: adding bi-gram\n",
    "    # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "    # >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "    Example: adding tri-gram\n",
    "    # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "    # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "    # >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "    \"\"\"\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Average train sequence length: 238\n",
      "Average test sequence length: 230\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 3s 112us/step - loss: 0.6170 - acc: 0.7280 - val_loss: 0.5125 - val_acc: 0.8145\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 3s 107us/step - loss: 0.4228 - acc: 0.8535 - val_loss: 0.3802 - val_acc: 0.8615\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 3s 102us/step - loss: 0.3311 - acc: 0.8796 - val_loss: 0.3284 - val_acc: 0.8745\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 3s 104us/step - loss: 0.2885 - acc: 0.8924 - val_loss: 0.3042 - val_acc: 0.8827\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 3s 104us/step - loss: 0.2621 - acc: 0.9028 - val_loss: 0.2909 - val_acc: 0.8844\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 3s 102us/step - loss: 0.2441 - acc: 0.9096 - val_loss: 0.2858 - val_acc: 0.8858\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 3s 102us/step - loss: 0.2292 - acc: 0.9144 - val_loss: 0.2814 - val_acc: 0.8869\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 3s 103us/step - loss: 0.2179 - acc: 0.9189 - val_loss: 0.2798 - val_acc: 0.8878\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 3s 109us/step - loss: 0.2085 - acc: 0.9227 - val_loss: 0.2839 - val_acc: 0.8854\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 3s 107us/step - loss: 0.2010 - acc: 0.9266 - val_loss: 0.2830 - val_acc: 0.8866\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 3s 105us/step - loss: 0.1941 - acc: 0.9292 - val_loss: 0.2860 - val_acc: 0.8860\n",
      "Test...\n",
      "Test data accuracy is  0.886\n"
     ]
    }
   ],
   "source": [
    "# ngram_range = 2 will add bi-grams features\n",
    "ngram_range = 1\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 20\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = FastText(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 增加2元词组信息 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Average train sequence length: 238\n",
      "Average test sequence length: 230\n",
      "Adding 2-gram features\n",
      "Average train sequence length: 476\n",
      "Average test sequence length: 443\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 225s 9ms/step - loss: 0.5864 - acc: 0.7738 - val_loss: 0.4424 - val_acc: 0.8516\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 242s 10ms/step - loss: 0.3090 - acc: 0.9133 - val_loss: 0.3088 - val_acc: 0.8870\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 243s 10ms/step - loss: 0.1746 - acc: 0.9564 - val_loss: 0.2670 - val_acc: 0.8978\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 243s 10ms/step - loss: 0.1053 - acc: 0.9766 - val_loss: 0.2474 - val_acc: 0.9019\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 242s 10ms/step - loss: 0.0643 - acc: 0.9895 - val_loss: 0.2404 - val_acc: 0.9032\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 243s 10ms/step - loss: 0.0397 - acc: 0.9952 - val_loss: 0.2383 - val_acc: 0.9036\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 240s 10ms/step - loss: 0.0246 - acc: 0.9976 - val_loss: 0.2410 - val_acc: 0.9037\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 233s 9ms/step - loss: 0.0155 - acc: 0.9987 - val_loss: 0.2448 - val_acc: 0.9046\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 232s 9ms/step - loss: 0.0099 - acc: 0.9995 - val_loss: 0.2516 - val_acc: 0.9034\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 240s 10ms/step - loss: 0.0064 - acc: 0.9996 - val_loss: 0.2572 - val_acc: 0.9030\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 275s 11ms/step - loss: 0.0042 - acc: 0.9998 - val_loss: 0.2658 - val_acc: 0.9020\n",
      "Test...\n",
      "Test data accuracy is  0.90204\n"
     ]
    }
   ],
   "source": [
    "ngram_range = 2\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 20\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "if ngram_range > 1:\n",
    "    print('Adding {}-gram features'.format(ngram_range))\n",
    "    # Create set of unique n-gram from the training set.\n",
    "    ngram_set = set()\n",
    "    for input_list in x_train:\n",
    "        for i in range(2, ngram_range + 1):\n",
    "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "            ngram_set.update(set_of_ngram)\n",
    "\n",
    "    # Dictionary mapping n-gram token to a unique integer.\n",
    "    # Integer values are greater than max_features in order\n",
    "    # to avoid collision with existing features.\n",
    "    start_index = max_features + 1\n",
    "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "    indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "    # max_features is the highest integer that could be found in the dataset.\n",
    "    max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "    # Augmenting x_train and x_test with n-grams features\n",
    "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = FastText(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
